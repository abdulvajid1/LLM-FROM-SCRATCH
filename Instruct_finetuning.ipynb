{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90045597",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/abdulvajid/AI/PROJECTS/LLM-FROM-SCRATCH/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_dataset\n",
    "\n",
    "from model import GPTModel\n",
    "from config import Config\n",
    "from load_weights_from_gpt import load_weights_into_gpt\n",
    "from gpt_download import download_and_load_gpt2\n",
    "from model_generate import generate\n",
    "import tiktoken\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e7a8199",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07f0849c",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config({\n",
    "    \"n_layers\": 24,\n",
    "    \"d_model\": 1024,\n",
    "    \"eps\": 1e-5,\n",
    "    \"hidden_size_multiplier\": 4,\n",
    "    \"num_heads\": 16,\n",
    "    \"context_len\": 1024,\n",
    "    \"dropout\": 0.01,\n",
    "    \"qkv_bias\": True,\n",
    "    \"vocab_size\": 50257\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b45a9e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ca2c267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device : cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"device : {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ecc784fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "77bcf65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPTModel(config=config).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "94706c6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "406336593\n"
     ]
    }
   ],
   "source": [
    "# total parameters\n",
    "total_params = 0\n",
    "for params in model.parameters():\n",
    "    total_params += params.numel()\n",
    "    \n",
    "print(total_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b044bf5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: gpt2/355M/checkpoint\n",
      "File already exists and is up-to-date: gpt2/355M/encoder.json\n",
      "File already exists and is up-to-date: gpt2/355M/hparams.json\n",
      "File already exists and is up-to-date: gpt2/355M/model.ckpt.data-00000-of-00001\n",
      "File already exists and is up-to-date: gpt2/355M/model.ckpt.index\n",
      "File already exists and is up-to-date: gpt2/355M/model.ckpt.meta\n",
      "File already exists and is up-to-date: gpt2/355M/vocab.bpe\n"
     ]
    }
   ],
   "source": [
    "settigns, params = download_and_load_gpt2(model_size='355M', models_dir='gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af1de9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_weights_into_gpt(model, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b5c12e5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fibonocci code in python: def fibonacci(n): \"\"\"Extract the number bounded by n from arithmetic expression IO'TIMING_RATE = b'((n % 3) % 5) % 4. This gets the scalar value of\n"
     ]
    }
   ],
   "source": [
    "print(generate(model, \n",
    "               starting_context='fibonocci code in python: def fibonacci(n):', \n",
    "               tokenizer=tokenizer, \n",
    "               max_len=40))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b33c8880",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset('teknium/openhermes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "99ef7dec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['output', 'input', 'instruction'],\n",
       "        num_rows: 242831\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "11a50d33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output': '```perl\\n#!/usr/bin/perl\\n\\nuse strict;\\nuse warnings;\\n\\n# Check if a log file path is provided as command-line argument\\nif (@ARGV != 1) {\\n    print \"Usage: $0 <log_file_path>\\\\n\";\\n    exit(1);\\n}\\n\\nmy $log_file = $ARGV[0];\\n\\n# Open the log file for reading\\nopen(my $fh, \\'<\\', $log_file) or die \"Could not open file \\'$log_file\\' $!\";\\n\\nmy %status_codes;\\n\\n# Process each line in the log file\\nwhile (my $line = <$fh>) {\\n    # Extract the HTTP status code from the log entry\\n    if ($line =~ /HTTP\\\\/\\\\d+\\\\.\\\\d+\" (\\\\d+)/) {\\n        my $status_code = $1;\\n        $status_codes{$status_code}++;\\n    }\\n}\\n\\nclose($fh);\\n\\n# Sort and print the results\\nforeach my $code (sort { $status_codes{$b} <=> $status_codes{$a} } keys %status_codes) {\\n    printf \"%s: %d\\\\n\", $code, $status_codes{$code};\\n}\\n```\\n\\nTo run this script, save it to a file named `count_http_status_codes.pl` and make sure it has executable permissions. Then, you can execute it with the following command:\\n\\n```\\n./count_http_status_codes.pl <path_to_log_file>\\n```\\n\\nReplace `<path_to_log_file>` with the actual path of your log file. The script will then output the count of different HTTP status codes found in the log file, sorted by frequency in descending order.',\n",
       " 'input': '',\n",
       " 'instruction': 'Write a Perl script that processes a log file and counts the occurrences of different HTTP status codes. The script should accept the log file path as a command-line argument and print the results to the console in descending order of frequency.\\n'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "691361db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_text_template(instr, input=''):\n",
    "    template = \"\"\"### Instruction: {instruction}\n",
    "### Input: {input}\n",
    "### Response:\"\"\"\n",
    "    return template.format(instruction=instr, input=input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e7f322f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_text_template(batch):\n",
    "    template = \"\"\"### Instruction: {instruction}\n",
    "### Input: {input}\n",
    "### Response: {output}\"\"\"\n",
    "    \n",
    "    return {'text': template.format(instruction=batch['instruction'], input=batch['input'], output=batch['output'])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d5b581e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction: Design a roller coaster with three distinct features, explaining the purpose of each feature and how it contributes to the overall ride experience.\n",
      "### Input: \n",
      "### Response: 1. The Gravity-Defying Loop: One of the most iconic and thrilling features of our roller coaster is a massive, vertical loop that takes riders upside down as they travel through it at high speeds. This gravity-defying loop creates an intense sensation of weightlessness and disorientation for riders, making them feel like they are defying the laws of physics. As the train enters the loop, riders experience strong positive G-forces pushing them into their seats, followed by brief moments of weightlessness at the top of the loop before being pushed back into their seats again as they exit the loop. This feature contributes to the overall ride experience by providing a heart-pounding moment of adrenaline and excitement that leaves riders wanting more.\n",
      "\n",
      "2. The Airtime Hills: To create a dynamic and varied ride experience, our roller coaster also includes a series of airtime hills â€“ undulating sections of track that cause riders to lift out of their seats briefly as they crest each hill. These hills provide a contrast to the intensity of the gravity-defying loop, offering moments of exhilaration and fun without the extreme forces experienced during the loop. As riders go over these hills, they experience negative G-forces, giving them the sensation of floating or flying momentarily before being pulled back down into their seats. This feature adds variety to the ride experience, ensuring that there's something for everyone to enjoy.\n",
      "\n",
      "3. The High-Speed Launch: Instead of a traditional chain lift hill, our roller coaster utilizes a state-of-the-art launch system that propels riders from 0 to 60 mph in just a few seconds. This high-speed launch provides an instant adrenaline rush and sets the tone for the rest of the ride, letting riders know that they're in for an unforgettable experience. By using this innovative technology, we can create a more seamless and immersive ride experience, eliminating the slow climb up a lift hill and replacing it with a sudden burst of speed that leaves riders breathless and exhilarated. This feature contributes to the overall ride experience by starting things off with a bang, setting the stage for the thrilling elements that follow.\n",
      "\n",
      "In conclusion, our roller coaster combines three distinct features â€“ the gravity-defying loop, airtime hills, and high-speed launch â€“ to create an unforgettable ride experience that offers something for everyone. Each feature serves a specific purpose in contributing to the overall excitement and enjoyment of the ride, ensuring that riders will want to come back for more time and time again.\n"
     ]
    }
   ],
   "source": [
    "# testing map function\n",
    "subset_data = data['train'].select([10,20,40])\n",
    "subset_data = subset_data.map(format_text_template)\n",
    "\n",
    "print(subset_data['text'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e6caf2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.map(format_text_template, remove_columns=['output','input','instruction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7dd7a0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize text in data\n",
    "def tokenize(batch):\n",
    "    token_ids = tokenizer.encode_batch(batch['text'], allowed_special={'<|endoftext|>'})\n",
    "    return {'input_ids': token_ids}   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ef872446",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = data.map(tokenize, batched=True, remove_columns=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "06f56b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtering any datapoint which have more that 1024 tokens\n",
    "dataset = dataset.filter(lambda x: len(x['input_ids']) <= 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8b6e3745",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'### Instruction: Write a Perl script that processes a log file and counts the occurrences of different HTTP status codes. The script should accept the log file path as a command-line argument and print the results to the console in descending order of frequency.\\n\\n### Input: \\n### Response: ```perl\\n#!/usr/bin/perl\\n\\nuse strict;\\nuse warnings;\\n\\n# Check if a log file path is provided as command-line argument\\nif (@ARGV != 1) {\\n    print \"Usage: $0 <log_file_path>\\\\n\";\\n    exit(1);\\n}\\n\\nmy $log_file = $ARGV[0];\\n\\n# Open the log file for reading\\nopen(my $fh, \\'<\\', $log_file) or die \"Could not open file \\'$log_file\\' $!\";\\n\\nmy %status_codes;\\n\\n# Process each line in the log file\\nwhile (my $line = <$fh>) {\\n    # Extract the HTTP status code from the log entry\\n    if ($line =~ /HTTP\\\\/\\\\d+\\\\.\\\\d+\" (\\\\d+)/) {\\n        my $status_code = $1;\\n        $status_codes{$status_code}++;\\n    }\\n}\\n\\nclose($fh);\\n\\n# Sort and print the results\\nforeach my $code (sort { $status_codes{$b} <=> $status_codes{$a} } keys %status_codes) {\\n    printf \"%s: %d\\\\n\", $code, $status_codes{$code};\\n}\\n```\\n\\nTo run this script, save it to a file named `count_http_status_codes.pl` and make sure it has executable permissions. Then, you can execute it with the following command:\\n\\n```\\n./count_http_status_codes.pl <path_to_log_file>\\n```\\n\\nReplace `<path_to_log_file>` with the actual path of your log file. The script will then output the count of different HTTP status codes found in the log file, sorted by frequency in descending order.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(dataset['train'][0]['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1a575d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset['train'].train_test_split(test_size=0.05, shuffle=True, seed=0)\n",
    "train_dataset = dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "accafc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = dataset['test'].train_test_split(test_size=0.4, seed=0)\n",
    "val_dataset = test_dataset['train']\n",
    "test_dataset = test_dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ad38004a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50256"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.eot_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2047dc1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def language_model_collate_fn(batch, max_len=None, tokenizer=tokenizer):\n",
    "    # find max len seq in the batch\n",
    "    pad_tok = tokenizer.eot_token\n",
    "    \n",
    "    batch = [seq['input_ids'] + [pad_tok] for seq in batch]\n",
    "    \n",
    "    batch_max_len = 0\n",
    "    for seq in batch:\n",
    "        if len(seq) > batch_max_len: # we give +1 since we have target token which need 1 token etc at end of token\n",
    "            batch_max_len = len(seq)\n",
    "            \n",
    "    \n",
    "    if max_len:\n",
    "        max_len = max(batch_max_len, max_len)\n",
    "    else:\n",
    "        max_len = batch_max_len\n",
    "    \n",
    "    # truncate to max len\n",
    "    batch = [seq[ :max_len] for seq in batch]\n",
    "        \n",
    "    # padding\n",
    "    batch = [seq + [pad_tok] * (max_len - len(seq)) for seq in batch]\n",
    "    \n",
    "    # langauge modeling input_ids and output_ids\n",
    "    input_ids = [seq[:-1] for seq in batch]\n",
    "    target_ids = [seq[1:] for seq in batch]\n",
    "    \n",
    "    return torch.tensor(input_ids), torch.tensor(target_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "accb071b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # testing collate function\n",
    "# language_model_collate_fn(batch=[[1,2,4,5,6],[2,3,3],[1]],\n",
    "#                           tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4766b647",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initilaize dataloader\n",
    "def create_dataloader(batch_size=4):\n",
    "    train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, \n",
    "                            collate_fn=language_model_collate_fn, \n",
    "                            pin_memory=False, shuffle=True, \n",
    "                            drop_last=True)\n",
    "\n",
    "    val_loader = DataLoader(dataset=val_dataset, batch_size=batch_size, \n",
    "                            collate_fn=language_model_collate_fn)\n",
    "\n",
    "    test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, \n",
    "                            collate_fn=language_model_collate_fn)\n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2338c69a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids'],\n",
       "    num_rows: 224148\n",
       "})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ab6c8677",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids'],\n",
       "    num_rows: 4720\n",
       "})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1354ff6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids'],\n",
       "    num_rows: 7078\n",
       "})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e9f738ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader, test_loader = create_dataloader(batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3bca3074",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for input_ids, target_ids in train_loader:\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "51261d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(tokenizer.decode(input_ids[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "27a87fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.eval();\n",
    "# model.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d1051e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # check input to model\n",
    "# with torch.no_grad():\n",
    "#     output = model(input_ids[0].unsqueeze(0).to('cuda'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "60224ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss on batch\n",
    "def calc_loss_on_batch(model, input_ids, target_ids):\n",
    "    logits = model(input_ids).flatten(0, 1)\n",
    "    target = target_ids.view(1, -1).squeeze()\n",
    "    loss = torch.nn.functional.cross_entropy(logits, target)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c3296664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cal loss on entire loader for eval\n",
    "def cal_loss_on_data_loader(model, data_loader, max_batch=None):\n",
    "    total_loss = 0\n",
    "    total_batch = len(data_loader)\n",
    "    \n",
    "    if max_batch:\n",
    "        max_batch = min(total_batch, max_batch)\n",
    "    else:\n",
    "        max_batch = total_batch\n",
    "    \n",
    "    for i, (input_ids, target_ids) in enumerate(data_loader):\n",
    "        if i == max_batch:\n",
    "            break\n",
    "        input_ids, target_ids = input_ids.to(device), target_ids.to(device)\n",
    "        loss = calc_loss_on_batch(model, input_ids, target_ids)\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    return total_loss / max_batch\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "426f58b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, train_loader, val_loader, max_batch):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = cal_loss_on_data_loader(model, train_loader, max_batch=max_batch)\n",
    "        val_loss = cal_loss_on_data_loader(model, val_loader, max_batch=max_batch)\n",
    "    model.train()\n",
    "    return train_loss, val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "66bb5fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model,\n",
    "            starting_context:str='what is the opposit of saying \"i love you\"',\n",
    "            tokenizer=tokenizer,\n",
    "            max_len=10,\n",
    "            sampling=True,\n",
    "            temperature=0.0,\n",
    "            top_k=None,\n",
    "            eos_id=None):\n",
    "    \n",
    "    \n",
    "    starting_context = apply_text_template(instr=starting_context, input='')\n",
    "    model.eval()\n",
    "    input_ids = tokenizer.encode(starting_context)\n",
    "    input_ids = torch.tensor(input_ids).unsqueeze(0).to(device)\n",
    "    \n",
    "    for i in range(max_len):\n",
    "        with torch.no_grad():\n",
    "            logits = model(input_ids)\n",
    "            logits = logits[:,-1,:]\n",
    "            \n",
    "            if sampling:\n",
    "                \n",
    "                if top_k:\n",
    "                    topk_logits, topk_pos = torch.topk(logits, k=top_k, dim=-1)\n",
    "                    logits = torch.where(input=torch.tensor(float('-inf')),\n",
    "                                         condition=logits < topk_logits[:,-1].reshape(-1, 1), \n",
    "                                         other=logits)\n",
    "                if temperature>0.0:\n",
    "                    logits = logits / temperature\n",
    "                    \n",
    "                probas = F.softmax(logits, dim=-1)\n",
    "                idx_next = torch.multinomial(probas, num_samples=1)\n",
    "                input_ids = torch.concat([input_ids, idx_next], dim=-1)\n",
    "            else:\n",
    "                assert temperature==0.0 and top_k is None, \"You can't set temperature or topk if sampling=False\"\n",
    "                last_token = torch.argmax(logits, dim=-1).unsqueeze(0)\n",
    "                input_ids = torch.cat([input_ids, last_token], dim=-1)\n",
    "    return tokenizer.decode(input_ids.squeeze().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "72456865",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5, weight_decay=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "99610422",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train(model, train_loader, val_loader, optimizer, eval_freq, save_model_epoch_freq=30, save_file='./'):\n",
    "#     global_step = 0\n",
    "#     train_loss_history = []\n",
    "#     val_loss_history = []\n",
    "#     model.to(device)\n",
    "    \n",
    "#     for epoch, (input_ids, target_ids) in enumerate(train_loader):\n",
    "#         model.train()\n",
    "#         optimizer.zero_grad()\n",
    "#         input_ids, target_ids = input_ids.to(device), target_ids.to(device)\n",
    "#         loss = calc_loss_on_batch(model, input_ids, target_ids)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "        \n",
    "#         global_step += 1\n",
    "        \n",
    "#         if global_step % eval_freq == 0 or epoch == 0:\n",
    "#             train_loss, val_loss = evaluate(model, train_loader, val_loader, max_batch=10)\n",
    "#             print(f\"epoch: {epoch} | training_loss: {train_loss} | val_loss: {val_loss}\")\n",
    "#             train_loss_history.append(train_loss)\n",
    "#             val_loss_history.append(val_loss)\n",
    "#             print(f\"Sample Generation\\n{'-'*20}\")\n",
    "#             print(generate(model))\n",
    "            \n",
    "    \n",
    "#     return train_loss_history,  val_loss_history\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "23c195bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_losses, val_losses = train(model, train_loader, val_loader, optimizer, eval_freq=5, save_model_epoch_freq=10, save_file='checkpoints')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2eb7021e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction: what is it\n",
      "### Input: \n",
      "### Response: mddieet ramwool figure / backed\n"
     ]
    }
   ],
   "source": [
    "out = generate(model, starting_context='what is it')\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5c8b3136",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader, optimizer, eval_freq, device, save_model_step_freq=10, save_file='runs', total_epoch=2):\n",
    "    \n",
    "    model.to(device)\n",
    "    \n",
    "    checkpoints_path = Path('runs')\n",
    "    checkpoints_path.mkdir(exist_ok=True)\n",
    "    model_checkpoints = list(checkpoints_path.glob(\"*.ckpt\"))\n",
    "    \n",
    "    if model_checkpoints:\n",
    "        last_save_path = model_checkpoints[-1].as_posix()\n",
    "        print(f'Loading last saved checkpoint : {last_save_path}')\n",
    "        training_state = torch.load(last_save_path)\n",
    "        global_step = training_state['global_step']\n",
    "        print(model.load_state_dict(training_state['model_state']))\n",
    "        optimizer.load_state_dict(training_state['optimizer_state'])\n",
    "        train_loss_history = training_state['train_loss_history']\n",
    "        val_loss_history = training_state['val_loss_history']\n",
    "        epoch = training_state['epoch']\n",
    "    else:\n",
    "        global_step = 0\n",
    "        train_loss_history = []\n",
    "        val_loss_history = []\n",
    "        epoch = 0\n",
    "        \n",
    "    print(f'current global_step: {global_step}')\n",
    "    print(f'current epoch: {epoch}')\n",
    "    if train_loss_history:\n",
    "        print(f'last loss: {train_loss_history[-1]} | val_loss: {val_loss_history[-1]}')\n",
    "    \n",
    "        \n",
    "    for epoch in range(epoch, total_epoch):\n",
    "        \n",
    "        for input_ids, target_ids in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            input_ids, target_ids = input_ids.to(device), target_ids.to(device)\n",
    "            loss = calc_loss_on_batch(model, input_ids, target_ids)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            global_step += 1    \n",
    "                \n",
    "            if global_step % (save_model_step_freq) == 0 :\n",
    "                print('saving model...')\n",
    "                torch.save({'model_state' : model.state_dict(),\n",
    "                            'optimizer_state': optimizer.state_dict(), \n",
    "                            'epoch': epoch,\n",
    "                            'global_step':global_step,\n",
    "                            'train_loss_history': train_loss_history,\n",
    "                            'val_loss_history': val_loss_history},\n",
    "                           (checkpoints_path / f\"checkpoint.ckpt\" ).as_posix())\n",
    "                print('save completed..')\n",
    "        \n",
    "            if global_step % eval_freq == 0 or epoch == 0:\n",
    "                train_loss, val_loss = evaluate(model, train_loader, val_loader, max_batch=10)\n",
    "                print(f\"global_step: {global_step} | training_loss: {train_loss} | val_loss: {val_loss}\")\n",
    "                train_loss_history.append(train_loss)\n",
    "                val_loss_history.append(val_loss)\n",
    "                print(f\"Sample Generation\\n{'-'*20}\")\n",
    "                print(generate(model, starting_context=\"what is the opposit of word 'love'\"))\n",
    "                \n",
    "            \n",
    "    \n",
    "    return train_loss_history,  val_loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0304f5b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading last saved checkpoint : runs/checkpoint.ckpt\n",
      "<All keys matched successfully>\n",
      "current global_step: 510\n",
      "current epoch: 5\n",
      "last loss: 1.9183476209640502 | val_loss: 1.5375691652297974\n",
      "saving model...\n",
      "save completed..\n",
      "global_step: 520 | training_loss: 1.7748191952705383 | val_loss: 1.5401386857032775\n",
      "Sample Generation\n",
      "--------------------\n",
      "### Instruction: what is the opposit of word 'love'\n",
      "### Input: \n",
      "### Response: 1. assertive\n",
      "2. possessive\n",
      "\n",
      "saving model...\n",
      "save completed..\n",
      "global_step: 530 | training_loss: 1.6150527238845824 | val_loss: 1.517040377855301\n",
      "Sample Generation\n",
      "--------------------\n",
      "### Instruction: what is the opposit of word 'love'\n",
      "### Input: \n",
      "### Response: love\n",
      "### Input: What is the opposites\n",
      "saving model...\n",
      "save completed..\n",
      "global_step: 540 | training_loss: 1.6841875731945037 | val_loss: 1.510129940509796\n",
      "Sample Generation\n",
      "--------------------\n",
      "### Instruction: what is the opposit of word 'love'\n",
      "### Input: \n",
      "### Response: Love is an essential development in a relationship. It\n",
      "saving model...\n",
      "save completed..\n",
      "global_step: 550 | training_loss: 1.77071373462677 | val_loss: 1.5124515652656556\n",
      "Sample Generation\n",
      "--------------------\n",
      "### Instruction: what is the opposit of word 'love'\n",
      "### Input: \n",
      "### Response: the common word \"love\" is used a lot\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[63], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_losses, val_losses \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mtotal_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43meval_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43msave_model_step_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43msave_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mruns\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[62], line 37\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, val_loader, optimizer, eval_freq, device, save_model_step_freq, save_file, total_epoch)\u001b[0m\n\u001b[1;32m     35\u001b[0m input_ids, target_ids \u001b[38;5;241m=\u001b[39m input_ids\u001b[38;5;241m.\u001b[39mto(device), target_ids\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     36\u001b[0m loss \u001b[38;5;241m=\u001b[39m calc_loss_on_batch(model, input_ids, target_ids)\n\u001b[0;32m---> 37\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     40\u001b[0m global_step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m    \n",
      "File \u001b[0;32m~/AI/PROJECTS/LLM-FROM-SCRATCH/.venv/lib/python3.9/site-packages/torch/_tensor.py:648\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    638\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    639\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    640\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    641\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    646\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    647\u001b[0m     )\n\u001b[0;32m--> 648\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    650\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/AI/PROJECTS/LLM-FROM-SCRATCH/.venv/lib/python3.9/site-packages/torch/autograd/__init__.py:353\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    348\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    350\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 353\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/AI/PROJECTS/LLM-FROM-SCRATCH/.venv/lib/python3.9/site-packages/torch/autograd/graph.py:824\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    822\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    823\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 824\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    825\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    826\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    827\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    828\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_losses, val_losses = train(model, \n",
    "                                 train_loader, \n",
    "                                 val_loader, \n",
    "                                 optimizer,\n",
    "                                 total_epoch=100, \n",
    "                                 eval_freq=10, \n",
    "                                 save_model_step_freq=10, \n",
    "                                 save_file='runs', \n",
    "                                 device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "89949d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"who is james bond?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "51954c09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction: who is james bond?\n",
      "### Input: \n",
      "### Response: James Bond is the title character of the James Bond film, Skyfall. Bond was the first and largest 007 to break into the world of finance. Bond joined the ranks of the super rich in his early years and became a millionaire by the time\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(generate(model, starting_context=question, max_len=50, temperature=0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "906212a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction: who the fuck is james bond, tell me answer in fuck\n",
      "### Input: \n",
      "### Response: James Bond is a British actor, best known for playing the role of James Bond in the original James Bond film. Bond is a spy who works for the British government and is known for his loyalty to his country.<|endoftext|>The following is a list of\n"
     ]
    }
   ],
   "source": [
    "print(generate(model, starting_context=question, max_len=50, sampling=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8609320f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
