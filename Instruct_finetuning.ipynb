{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90045597",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_dataset\n",
    "\n",
    "from model import GPTModel\n",
    "from config import Config\n",
    "from load_weights_from_gpt import load_weights_into_gpt\n",
    "from gpt_download import download_and_load_gpt2\n",
    "from model_generate import generate\n",
    "import tiktoken\n",
    "\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e7a8199",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07f0849c",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config({\n",
    "    \"n_layers\": 24,\n",
    "    \"d_model\": 1024,\n",
    "    \"eps\": 1e-5,\n",
    "    \"hidden_size_multiplier\": 4,\n",
    "    \"num_heads\": 16,\n",
    "    \"context_len\": 1024,\n",
    "    \"dropout\": 0.01,\n",
    "    \"qkv_bias\": True,\n",
    "    \"vocab_size\": 50257\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b45a9e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ca2c267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device : cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"device : {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ecc784fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "77bcf65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPTModel(config=config).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b044bf5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: gpt2\\355M\\checkpoint\n",
      "File already exists and is up-to-date: gpt2\\355M\\encoder.json\n",
      "File already exists and is up-to-date: gpt2\\355M\\hparams.json\n",
      "File already exists and is up-to-date: gpt2\\355M\\model.ckpt.data-00000-of-00001\n",
      "File already exists and is up-to-date: gpt2\\355M\\model.ckpt.index\n",
      "File already exists and is up-to-date: gpt2\\355M\\model.ckpt.meta\n",
      "File already exists and is up-to-date: gpt2\\355M\\vocab.bpe\n"
     ]
    }
   ],
   "source": [
    "settigns, params = download_and_load_gpt2(model_size='355M', models_dir='gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af1de9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_weights_into_gpt(model, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5c12e5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fibonocci code in python: def fibonacci(n): if n >= 1: return results[y*n*n+1:] if n < 0: return garbage.Repr() text = `` `` print fibonacci.joined.text()\n"
     ]
    }
   ],
   "source": [
    "print(generate(model, \n",
    "               starting_context='fibonocci code in python: def fibonacci(n):', \n",
    "               tokenizer=tokenizer, \n",
    "               max_len=40))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b33c8880",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset('teknium/openhermes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "99ef7dec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['output', 'input', 'instruction'],\n",
       "        num_rows: 242831\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "11a50d33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output': '```perl\\n#!/usr/bin/perl\\n\\nuse strict;\\nuse warnings;\\n\\n# Check if a log file path is provided as command-line argument\\nif (@ARGV != 1) {\\n    print \"Usage: $0 <log_file_path>\\\\n\";\\n    exit(1);\\n}\\n\\nmy $log_file = $ARGV[0];\\n\\n# Open the log file for reading\\nopen(my $fh, \\'<\\', $log_file) or die \"Could not open file \\'$log_file\\' $!\";\\n\\nmy %status_codes;\\n\\n# Process each line in the log file\\nwhile (my $line = <$fh>) {\\n    # Extract the HTTP status code from the log entry\\n    if ($line =~ /HTTP\\\\/\\\\d+\\\\.\\\\d+\" (\\\\d+)/) {\\n        my $status_code = $1;\\n        $status_codes{$status_code}++;\\n    }\\n}\\n\\nclose($fh);\\n\\n# Sort and print the results\\nforeach my $code (sort { $status_codes{$b} <=> $status_codes{$a} } keys %status_codes) {\\n    printf \"%s: %d\\\\n\", $code, $status_codes{$code};\\n}\\n```\\n\\nTo run this script, save it to a file named `count_http_status_codes.pl` and make sure it has executable permissions. Then, you can execute it with the following command:\\n\\n```\\n./count_http_status_codes.pl <path_to_log_file>\\n```\\n\\nReplace `<path_to_log_file>` with the actual path of your log file. The script will then output the count of different HTTP status codes found in the log file, sorted by frequency in descending order.',\n",
       " 'input': '',\n",
       " 'instruction': 'Write a Perl script that processes a log file and counts the occurrences of different HTTP status codes. The script should accept the log file path as a command-line argument and print the results to the console in descending order of frequency.\\n'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "691361db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_text_template(instr, input=''):\n",
    "    template = \"\"\"### Instruction: {instruction}\n",
    "### Input: {input}\n",
    "### Response:\"\"\"\n",
    "    return template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e7f322f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_text_template(batch):\n",
    "    template = \"\"\"### Instruction: {instruction}\n",
    "### Input: {input}\n",
    "### Response: {output}\"\"\"\n",
    "    \n",
    "    return {'text': template.format(instruction=batch['instruction'], input=batch['input'], output=batch['output'])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d5b581e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction: Design a roller coaster with three distinct features, explaining the purpose of each feature and how it contributes to the overall ride experience.\n",
      "### Input: \n",
      "### Response: 1. The Gravity-Defying Loop: One of the most iconic and thrilling features of our roller coaster is a massive, vertical loop that takes riders upside down as they travel through it at high speeds. This gravity-defying loop creates an intense sensation of weightlessness and disorientation for riders, making them feel like they are defying the laws of physics. As the train enters the loop, riders experience strong positive G-forces pushing them into their seats, followed by brief moments of weightlessness at the top of the loop before being pushed back into their seats again as they exit the loop. This feature contributes to the overall ride experience by providing a heart-pounding moment of adrenaline and excitement that leaves riders wanting more.\n",
      "\n",
      "2. The Airtime Hills: To create a dynamic and varied ride experience, our roller coaster also includes a series of airtime hills â€“ undulating sections of track that cause riders to lift out of their seats briefly as they crest each hill. These hills provide a contrast to the intensity of the gravity-defying loop, offering moments of exhilaration and fun without the extreme forces experienced during the loop. As riders go over these hills, they experience negative G-forces, giving them the sensation of floating or flying momentarily before being pulled back down into their seats. This feature adds variety to the ride experience, ensuring that there's something for everyone to enjoy.\n",
      "\n",
      "3. The High-Speed Launch: Instead of a traditional chain lift hill, our roller coaster utilizes a state-of-the-art launch system that propels riders from 0 to 60 mph in just a few seconds. This high-speed launch provides an instant adrenaline rush and sets the tone for the rest of the ride, letting riders know that they're in for an unforgettable experience. By using this innovative technology, we can create a more seamless and immersive ride experience, eliminating the slow climb up a lift hill and replacing it with a sudden burst of speed that leaves riders breathless and exhilarated. This feature contributes to the overall ride experience by starting things off with a bang, setting the stage for the thrilling elements that follow.\n",
      "\n",
      "In conclusion, our roller coaster combines three distinct features â€“ the gravity-defying loop, airtime hills, and high-speed launch â€“ to create an unforgettable ride experience that offers something for everyone. Each feature serves a specific purpose in contributing to the overall excitement and enjoyment of the ride, ensuring that riders will want to come back for more time and time again.\n"
     ]
    }
   ],
   "source": [
    "# testing map function\n",
    "subset_data = data['train'].select([10,20,40])\n",
    "subset_data = subset_data.map(format_text_template)\n",
    "\n",
    "print(subset_data['text'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e6caf2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.map(format_text_template, remove_columns=['output','input','instruction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7dd7a0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize text in data\n",
    "def tokenize(batch):\n",
    "    token_ids = tokenizer.encode_batch(batch['text'], allowed_special={'<|endoftext|>'})\n",
    "    return {'input_ids': token_ids}   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ef872446",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = data.map(tokenize, batched=True, remove_columns=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "06f56b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtering any datapoint which have more that 1024 tokens\n",
    "dataset = dataset.filter(lambda x: len(x['input_ids']) <= 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8b6e3745",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'### Instruction: Write a Perl script that processes a log file and counts the occurrences of different HTTP status codes. The script should accept the log file path as a command-line argument and print the results to the console in descending order of frequency.\\n\\n### Input: \\n### Response: ```perl\\n#!/usr/bin/perl\\n\\nuse strict;\\nuse warnings;\\n\\n# Check if a log file path is provided as command-line argument\\nif (@ARGV != 1) {\\n    print \"Usage: $0 <log_file_path>\\\\n\";\\n    exit(1);\\n}\\n\\nmy $log_file = $ARGV[0];\\n\\n# Open the log file for reading\\nopen(my $fh, \\'<\\', $log_file) or die \"Could not open file \\'$log_file\\' $!\";\\n\\nmy %status_codes;\\n\\n# Process each line in the log file\\nwhile (my $line = <$fh>) {\\n    # Extract the HTTP status code from the log entry\\n    if ($line =~ /HTTP\\\\/\\\\d+\\\\.\\\\d+\" (\\\\d+)/) {\\n        my $status_code = $1;\\n        $status_codes{$status_code}++;\\n    }\\n}\\n\\nclose($fh);\\n\\n# Sort and print the results\\nforeach my $code (sort { $status_codes{$b} <=> $status_codes{$a} } keys %status_codes) {\\n    printf \"%s: %d\\\\n\", $code, $status_codes{$code};\\n}\\n```\\n\\nTo run this script, save it to a file named `count_http_status_codes.pl` and make sure it has executable permissions. Then, you can execute it with the following command:\\n\\n```\\n./count_http_status_codes.pl <path_to_log_file>\\n```\\n\\nReplace `<path_to_log_file>` with the actual path of your log file. The script will then output the count of different HTTP status codes found in the log file, sorted by frequency in descending order.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(dataset['train'][0]['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1a575d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset['train'].train_test_split(test_size=0.05, shuffle=True, seed=0)\n",
    "train_dataset = dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "accafc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = dataset['test'].train_test_split(test_size=0.4, seed=0)\n",
    "val_dataset = test_dataset['train']\n",
    "test_dataset = test_dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ad38004a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50256"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.eot_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2047dc1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def language_model_collate_fn(batch, max_len=None, tokenizer=tokenizer):\n",
    "    # find max len seq in the batch\n",
    "    pad_tok = tokenizer.eot_token\n",
    "    \n",
    "    batch = [seq['input_ids'] + [pad_tok] for seq in batch]\n",
    "    \n",
    "    batch_max_len = 0\n",
    "    for seq in batch:\n",
    "        if len(seq) > batch_max_len: # we give +1 since we have target token which need 1 token etc at end of token\n",
    "            batch_max_len = len(seq)\n",
    "            \n",
    "    \n",
    "    if max_len:\n",
    "        max_len = max(batch_max_len, max_len)\n",
    "    else:\n",
    "        max_len = batch_max_len\n",
    "    \n",
    "    # truncate to max len\n",
    "    batch = [seq[ :max_len] for seq in batch]\n",
    "        \n",
    "    # padding\n",
    "    batch = [seq + [pad_tok] * (max_len - len(seq)) for seq in batch]\n",
    "    \n",
    "    # langauge modeling input_ids and output_ids\n",
    "    input_ids = [seq[:-1] for seq in batch]\n",
    "    target_ids = [seq[1:] for seq in batch]\n",
    "    \n",
    "    return torch.tensor(input_ids), torch.tensor(target_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "accb071b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # testing collate function\n",
    "# language_model_collate_fn(batch=[[1,2,4,5,6],[2,3,3],[1]],\n",
    "#                           tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4766b647",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initilaize dataloader\n",
    "def create_dataloader(batch_size=4):\n",
    "    train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, \n",
    "                            collate_fn=language_model_collate_fn, \n",
    "                            pin_memory=True, shuffle=True, \n",
    "                            drop_last=True)\n",
    "\n",
    "    val_loader = DataLoader(dataset=val_dataset, batch_size=8, \n",
    "                            collate_fn=language_model_collate_fn)\n",
    "\n",
    "    test_loader = DataLoader(dataset=test_dataset, batch_size=8, \n",
    "                            collate_fn=language_model_collate_fn)\n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2338c69a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids'],\n",
       "    num_rows: 224148\n",
       "})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ab6c8677",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids'],\n",
       "    num_rows: 4720\n",
       "})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1354ff6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids'],\n",
       "    num_rows: 7078\n",
       "})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e9f738ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader, test_loader = create_dataloader(batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3bca3074",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for input_ids, target_ids in train_loader:\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "51261d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(tokenizer.decode(input_ids[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "27a87fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.eval();\n",
    "# model.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d1051e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # check input to model\n",
    "# with torch.no_grad():\n",
    "#     output = model(input_ids[0].unsqueeze(0).to('cuda'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "60224ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss on batch\n",
    "def calc_loss_on_batch(model, input_ids, target_ids):\n",
    "    logits = model(input_ids).flatten(0, 1)\n",
    "    target = target_ids.view(1, -1).squeeze()\n",
    "    loss = torch.nn.functional.cross_entropy(logits, target)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c3296664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cal loss on entire loader for eval\n",
    "def cal_loss_on_data_loader(model, data_loader, max_batch=None):\n",
    "    total_loss = 0\n",
    "    total_batch = len(data_loader)\n",
    "    \n",
    "    if max_batch:\n",
    "        max_batch = min(total_batch, max_batch)\n",
    "    else:\n",
    "        max_batch = total_batch\n",
    "    \n",
    "    for i, (input_ids, target_ids) in enumerate(data_loader):\n",
    "        if i == max_batch:\n",
    "            break\n",
    "        input_ids, target_ids = input_ids.to(device), target_ids.to(device)\n",
    "        loss = calc_loss_on_batch(model, input_ids, target_ids)\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    return total_loss / max_batch\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "426f58b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, train_loader, val_loader, max_batch):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = cal_loss_on_data_loader(model, train_loader, max_batch=max_batch)\n",
    "        val_loss = cal_loss_on_data_loader(model, val_loader, max_batch=max_batch)\n",
    "    model.train()\n",
    "    return train_loss, val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "66bb5fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model,\n",
    "            starting_context:str='what is the opposit of saying \"i love you\"',\n",
    "            tokenizer=tokenizer,\n",
    "            max_len=10,\n",
    "            sampling=True,\n",
    "            temperature=0.0,\n",
    "            top_k=None,\n",
    "            eos_id=None):\n",
    "    \n",
    "    \n",
    "    starting_context = apply_text_template(starting_context, input='')\n",
    "    model.eval()\n",
    "    input_ids = tokenizer.encode(starting_context)\n",
    "    input_ids = torch.tensor(input_ids).unsqueeze(0).to(device)\n",
    "    \n",
    "    for i in range(max_len):\n",
    "        with torch.no_grad():\n",
    "            logits = model(input_ids)\n",
    "            logits = logits[:,-1,:]\n",
    "            \n",
    "            if sampling:\n",
    "                \n",
    "                if top_k:\n",
    "                    topk_logits, topk_pos = torch.topk(logits, k=top_k, dim=-1)\n",
    "                    logits = torch.where(input=torch.tensor(float('-inf')),\n",
    "                                         condition=logits < topk_logits[:,-1].reshape(-1, 1), \n",
    "                                         other=logits)\n",
    "                if temperature>0.0:\n",
    "                    logits = logits / temperature\n",
    "                    \n",
    "                probas = F.softmax(logits, dim=-1)\n",
    "                idx_next = torch.multinomial(probas, num_samples=1)\n",
    "                input_ids = torch.concat([input_ids, idx_next], dim=-1)\n",
    "            else:\n",
    "                assert temperature==0.0 and top_k is None, \"You can't set temperature or topk if sampling=False\"\n",
    "                last_token = torch.argmax(logits, dim=-1).unsqueeze(0)\n",
    "                input_ids = torch.cat([input_ids, last_token], dim=-1)\n",
    "    return tokenizer.decode(input_ids.squeeze().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "72456865",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5, weight_decay=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "99610422",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader, optimizer, eval_freq):\n",
    "    global_step = 0\n",
    "    train_loss_history = []\n",
    "    val_loss_history = []\n",
    "    model.to(device)\n",
    "    for epoch, (input_ids, target_ids) in enumerate(train_loader):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        input_ids, target_ids = input_ids.to(device), target_ids.to(device)\n",
    "        loss = calc_loss_on_batch(model, input_ids, target_ids)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        global_step += 1\n",
    "        \n",
    "        if global_step % eval_freq == 0 or epoch == 0:\n",
    "            train_loss, val_loss = evaluate(model, train_loader, val_loader, max_batch=100)\n",
    "            print(f\"epoch: {epoch} | training_loss: {train_loss} | val_loss: {val_loss}\")\n",
    "            train_loss_history.append(train_loss)\n",
    "            val_loss_history.append(val_loss)\n",
    "            print(f\"Sample Generation\\n{'-'*20}\")\n",
    "            print(generate(model))\n",
    "            \n",
    "    \n",
    "    return train_loss_history,  val_loss_history\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "23c195bd",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[42]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m train_losses, val_losses = \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_freq\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[41]\u001b[39m\u001b[32m, line 17\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(model, train_loader, val_loader, optimizer, eval_freq)\u001b[39m\n\u001b[32m     14\u001b[39m global_step += \u001b[32m1\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m global_step % eval_freq == \u001b[32m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m epoch == \u001b[32m0\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m     train_loss, val_loss = \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_batch\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mepoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m | training_loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m | val_loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     19\u001b[39m     train_loss_history.append(train_loss)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[38]\u001b[39m\u001b[32m, line 5\u001b[39m, in \u001b[36mevaluate\u001b[39m\u001b[34m(model, train_loader, val_loader, max_batch)\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m      4\u001b[39m     train_loss = cal_loss_on_data_loader(model, train_loader, max_batch=max_batch)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     val_loss = \u001b[43mcal_loss_on_data_loader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_batch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m model.train()\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m train_loss, val_loss\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 15\u001b[39m, in \u001b[36mcal_loss_on_data_loader\u001b[39m\u001b[34m(model, data_loader, max_batch)\u001b[39m\n\u001b[32m     13\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m     14\u001b[39m     input_ids, target_ids = input_ids.to(device), target_ids.to(device)\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m     loss = \u001b[43mcalc_loss_on_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m     total_loss += loss.item()\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m total_loss / max_batch\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 5\u001b[39m, in \u001b[36mcalc_loss_on_batch\u001b[39m\u001b[34m(model, input_ids, target_ids)\u001b[39m\n\u001b[32m      3\u001b[39m logits = model(input_ids).flatten(\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m)\n\u001b[32m      4\u001b[39m target = target_ids.view(\u001b[32m1\u001b[39m, -\u001b[32m1\u001b[39m).squeeze()\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m loss = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfunctional\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Machine Learning\\PROJECTS\\NOT COMPLETED\\LLM-FROM-SCRATCH\\.venv\\Lib\\site-packages\\torch\\nn\\functional.py:3494\u001b[39m, in \u001b[36mcross_entropy\u001b[39m\u001b[34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[39m\n\u001b[32m   3492\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   3493\u001b[39m     reduction = _Reduction.legacy_get_string(size_average, reduce)\n\u001b[32m-> \u001b[39m\u001b[32m3494\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_C\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_nn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3495\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   3496\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3497\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3498\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3499\u001b[39m \u001b[43m    \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3500\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3501\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "train_losses, val_losses = train(model, train_loader, val_loader, optimizer, eval_freq=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686a54b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
