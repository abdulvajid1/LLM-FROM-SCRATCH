{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "90045597",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_dataset\n",
    "\n",
    "from model import GPTModel\n",
    "from config import Config\n",
    "from load_weights_from_gpt import load_weights_into_gpt\n",
    "from gpt_download import download_and_load_gpt2\n",
    "from model_generate import generate\n",
    "import tiktoken\n",
    "\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07f0849c",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config({\n",
    "    \"n_layers\": 24,\n",
    "    \"d_model\": 1024,\n",
    "    \"eps\": 1e-5,\n",
    "    \"hidden_size_multiplier\": 4,\n",
    "    \"num_heads\": 16,\n",
    "    \"context_len\": 1024,\n",
    "    \"dropout\": 0.01,\n",
    "    \"qkv_bias\": True,\n",
    "    \"vocab_size\": 50257\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b45a9e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ca2c267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device : cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"device : {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ecc784fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77bcf65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPTModel(config=config).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b044bf5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: gpt2\\355M\\checkpoint\n",
      "File already exists and is up-to-date: gpt2\\355M\\encoder.json\n",
      "File already exists and is up-to-date: gpt2\\355M\\hparams.json\n",
      "File already exists and is up-to-date: gpt2\\355M\\model.ckpt.data-00000-of-00001\n",
      "File already exists and is up-to-date: gpt2\\355M\\model.ckpt.index\n",
      "File already exists and is up-to-date: gpt2\\355M\\model.ckpt.meta\n",
      "File already exists and is up-to-date: gpt2\\355M\\vocab.bpe\n"
     ]
    }
   ],
   "source": [
    "settigns, params = download_and_load_gpt2(model_size='355M', models_dir='gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af1de9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_weights_into_gpt(model, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b5c12e5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fibonocci code in python: def fibonacci(n): \"\"\" creates an infinite loop by square 1\"\"\"for i in range(100, N): circled += Ncatches=(i+0.5*i+100) how long += 1return '\n"
     ]
    }
   ],
   "source": [
    "print(generate(model, \n",
    "               starting_context='fibonocci code in python: def fibonacci(n):', \n",
    "               tokenizer=tokenizer, \n",
    "               max_len=40))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b33c8880",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset('teknium/openhermes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "99ef7dec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['output', 'input', 'instruction'],\n",
       "        num_rows: 242831\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "11a50d33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output': '```perl\\n#!/usr/bin/perl\\n\\nuse strict;\\nuse warnings;\\n\\n# Check if a log file path is provided as command-line argument\\nif (@ARGV != 1) {\\n    print \"Usage: $0 <log_file_path>\\\\n\";\\n    exit(1);\\n}\\n\\nmy $log_file = $ARGV[0];\\n\\n# Open the log file for reading\\nopen(my $fh, \\'<\\', $log_file) or die \"Could not open file \\'$log_file\\' $!\";\\n\\nmy %status_codes;\\n\\n# Process each line in the log file\\nwhile (my $line = <$fh>) {\\n    # Extract the HTTP status code from the log entry\\n    if ($line =~ /HTTP\\\\/\\\\d+\\\\.\\\\d+\" (\\\\d+)/) {\\n        my $status_code = $1;\\n        $status_codes{$status_code}++;\\n    }\\n}\\n\\nclose($fh);\\n\\n# Sort and print the results\\nforeach my $code (sort { $status_codes{$b} <=> $status_codes{$a} } keys %status_codes) {\\n    printf \"%s: %d\\\\n\", $code, $status_codes{$code};\\n}\\n```\\n\\nTo run this script, save it to a file named `count_http_status_codes.pl` and make sure it has executable permissions. Then, you can execute it with the following command:\\n\\n```\\n./count_http_status_codes.pl <path_to_log_file>\\n```\\n\\nReplace `<path_to_log_file>` with the actual path of your log file. The script will then output the count of different HTTP status codes found in the log file, sorted by frequency in descending order.',\n",
       " 'input': '',\n",
       " 'instruction': 'Write a Perl script that processes a log file and counts the occurrences of different HTTP status codes. The script should accept the log file path as a command-line argument and print the results to the console in descending order of frequency.\\n'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e7f322f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_text_template(batch):\n",
    "    template = \"\"\"### Instruction: {instruction}\n",
    "### Input: {input}\n",
    "### Response: {output}\"\"\"\n",
    "    \n",
    "    return {'text': template.format(instruction=batch['instruction'], input=batch['input'], output=batch['output'])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d5b581e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction: Design a roller coaster with three distinct features, explaining the purpose of each feature and how it contributes to the overall ride experience.\n",
      "### Input: \n",
      "### Response: 1. The Gravity-Defying Loop: One of the most iconic and thrilling features of our roller coaster is a massive, vertical loop that takes riders upside down as they travel through it at high speeds. This gravity-defying loop creates an intense sensation of weightlessness and disorientation for riders, making them feel like they are defying the laws of physics. As the train enters the loop, riders experience strong positive G-forces pushing them into their seats, followed by brief moments of weightlessness at the top of the loop before being pushed back into their seats again as they exit the loop. This feature contributes to the overall ride experience by providing a heart-pounding moment of adrenaline and excitement that leaves riders wanting more.\n",
      "\n",
      "2. The Airtime Hills: To create a dynamic and varied ride experience, our roller coaster also includes a series of airtime hills – undulating sections of track that cause riders to lift out of their seats briefly as they crest each hill. These hills provide a contrast to the intensity of the gravity-defying loop, offering moments of exhilaration and fun without the extreme forces experienced during the loop. As riders go over these hills, they experience negative G-forces, giving them the sensation of floating or flying momentarily before being pulled back down into their seats. This feature adds variety to the ride experience, ensuring that there's something for everyone to enjoy.\n",
      "\n",
      "3. The High-Speed Launch: Instead of a traditional chain lift hill, our roller coaster utilizes a state-of-the-art launch system that propels riders from 0 to 60 mph in just a few seconds. This high-speed launch provides an instant adrenaline rush and sets the tone for the rest of the ride, letting riders know that they're in for an unforgettable experience. By using this innovative technology, we can create a more seamless and immersive ride experience, eliminating the slow climb up a lift hill and replacing it with a sudden burst of speed that leaves riders breathless and exhilarated. This feature contributes to the overall ride experience by starting things off with a bang, setting the stage for the thrilling elements that follow.\n",
      "\n",
      "In conclusion, our roller coaster combines three distinct features – the gravity-defying loop, airtime hills, and high-speed launch – to create an unforgettable ride experience that offers something for everyone. Each feature serves a specific purpose in contributing to the overall excitement and enjoyment of the ride, ensuring that riders will want to come back for more time and time again.\n"
     ]
    }
   ],
   "source": [
    "# testing map function\n",
    "subset_data = data['train'].select([10,20,40])\n",
    "subset_data = subset_data.map(format_text_template)\n",
    "\n",
    "print(subset_data['text'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e6caf2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.map(format_text_template, remove_columns=['output','input','instruction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7dd7a0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize text in data\n",
    "def tokenize(batch):\n",
    "    token_ids = tokenizer.encode_batch(batch['text'], allowed_special={'<|endoftext|>'})\n",
    "    return {'input_ids': token_ids}   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef872446",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = data.map(tokenize, batched=True, remove_columns=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8b6e3745",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'### Instruction: Write a Perl script that processes a log file and counts the occurrences of different HTTP status codes. The script should accept the log file path as a command-line argument and print the results to the console in descending order of frequency.\\n\\n### Input: \\n### Response: ```perl\\n#!/usr/bin/perl\\n\\nuse strict;\\nuse warnings;\\n\\n# Check if a log file path is provided as command-line argument\\nif (@ARGV != 1) {\\n    print \"Usage: $0 <log_file_path>\\\\n\";\\n    exit(1);\\n}\\n\\nmy $log_file = $ARGV[0];\\n\\n# Open the log file for reading\\nopen(my $fh, \\'<\\', $log_file) or die \"Could not open file \\'$log_file\\' $!\";\\n\\nmy %status_codes;\\n\\n# Process each line in the log file\\nwhile (my $line = <$fh>) {\\n    # Extract the HTTP status code from the log entry\\n    if ($line =~ /HTTP\\\\/\\\\d+\\\\.\\\\d+\" (\\\\d+)/) {\\n        my $status_code = $1;\\n        $status_codes{$status_code}++;\\n    }\\n}\\n\\nclose($fh);\\n\\n# Sort and print the results\\nforeach my $code (sort { $status_codes{$b} <=> $status_codes{$a} } keys %status_codes) {\\n    printf \"%s: %d\\\\n\", $code, $status_codes{$code};\\n}\\n```\\n\\nTo run this script, save it to a file named `count_http_status_codes.pl` and make sure it has executable permissions. Then, you can execute it with the following command:\\n\\n```\\n./count_http_status_codes.pl <path_to_log_file>\\n```\\n\\nReplace `<path_to_log_file>` with the actual path of your log file. The script will then output the count of different HTTP status codes found in the log file, sorted by frequency in descending order.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(dataset['train'][0]['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "accafc13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'input_ids'],\n",
       "        num_rows: 242831\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ad38004a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50256"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.eot_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2047dc1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def language_model_collate_fn(batch, max_len=None, tokenizer=tokenizer):\n",
    "    # find max len seq in the batch\n",
    "    pad_tok = tokenizer.eot_token\n",
    "    new_batch = []\n",
    "    final_batch = []\n",
    "    \n",
    "    batch = [seq + [pad_tok] for seq in batch]\n",
    "    \n",
    "    batch_max_len = 0\n",
    "    for seq in batch:\n",
    "        if len(seq) > batch_max_len: # we give +1 since we have target token which need 1 token etc at end of token\n",
    "            batch_max_len = len(seq)\n",
    "            \n",
    "    \n",
    "    if max_len:\n",
    "        max_len = max(batch_max_len, max_len)\n",
    "    else:\n",
    "        max_len = batch_max_len\n",
    "    \n",
    "    # truncate to max len\n",
    "    batch = [seq[ :max_len] for seq in batch]\n",
    "        \n",
    "    # padding\n",
    "    batch = [seq + [pad_tok] * (max_len - len(seq)) for seq in batch]\n",
    "    \n",
    "    # langauge modeling input_ids and output_ids\n",
    "    input_ids = [seq[:-1] for seq in batch]\n",
    "    target_ids = [seq[1:] for seq in batch]\n",
    "    \n",
    "    return torch.tensor(input_ids), torch.tensor(target_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "accb071b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[    1,     2,     4,     5,     6],\n",
       "         [    2,     3,     3, 50256, 50256],\n",
       "         [    1, 50256, 50256, 50256, 50256]]),\n",
       " tensor([[    2,     4,     5,     6, 50256],\n",
       "         [    3,     3, 50256, 50256, 50256],\n",
       "         [50256, 50256, 50256, 50256, 50256]]))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing collate function\n",
    "language_model_collate_fn(batch=[[1,2,4,5,6],[2,3,3],[1]],\n",
    "                          tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4766b647",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initilaize dataloader\n",
    "train_loader = DataLoader(dataset=dataset['train'], batch_size=8, \n",
    "                          collate_fn=language_model_collate_fn, \n",
    "                          pin_memory=True, shuffle=True, \n",
    "                          drop_last=True)\n",
    "\n",
    "val_loader = DataLoader(dataset=dataset['val'], batch_size=8, \n",
    "                          collate_fn=language_model_collate_fn)\n",
    "\n",
    "test_loader = DataLoader(dataset=dataset['test'], batch_size=8, \n",
    "                          collate_fn=language_model_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2338c69a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
