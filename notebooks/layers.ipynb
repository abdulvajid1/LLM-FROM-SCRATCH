{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4286271",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dataclasses import dataclass\n",
    "import math\n",
    "from IPython.display import display, Image\n",
    "import tiktoken\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim import AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98816c78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F81c2aa73-dd8c-46bf-85b0-90e01145b0ed_1422x1460.png\" width=\"400\" height=\"500\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "url = \"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F81c2aa73-dd8c-46bf-85b0-90e01145b0ed_1422x1460.png\"\n",
    "display(Image(url=url,\n",
    "              width=400,\n",
    "              height=500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bbda22d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    n_layers:int = 12\n",
    "    d_model:int = 768\n",
    "    eps:float = 1e-5\n",
    "    hidden_size_multiplier:int = 4 \n",
    "    num_heads:int = 12\n",
    "    context_len:int = 1024\n",
    "    dropout:float = 0.1\n",
    "    qkv_bias:bool = False\n",
    "    vocab_size:int = 50257\n",
    "    \n",
    "config = Config()\n",
    "device = torch.device('mps' if torch.mps.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e1aef07",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7368fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer Normalization layer\n",
    "class LayerNormalization(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.eps = config.eps\n",
    "        self.scale = nn.Parameter(torch.ones((config.d_model)))\n",
    "        self.shift = nn.Parameter(torch.zeros((config.d_model))) \n",
    "        \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x_mean = x.mean(dim=-1, keepdim=True) \n",
    "        x_std = x.std(dim=-1, keepdim=True)\n",
    "        x_norm = (x - x_mean) / (x_std + self.eps)\n",
    "        return x_norm * self.scale + self.shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60072785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FeedForward Layer\n",
    "class FeedForwardLayer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        d_model = config.d_model\n",
    "        hidden_size_multiplier = config.hidden_size_multiplier\n",
    "        \n",
    "        self.ff_layer = nn.Sequential(\n",
    "            nn.Linear(d_model, hidden_size_multiplier * d_model),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_size_multiplier * d_model, d_model)        \n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.ff_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ac3000d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttentionLayer(nn.Module):\n",
    "    def __init__(self, config: Config):\n",
    "        super().__init__()\n",
    "        self.query_weights = nn.Linear(config.d_model, config.d_model, bias=config.qkv_bias)\n",
    "        self.key_weights = nn.Linear(config.d_model, config.d_model, bias=config.qkv_bias)\n",
    "        self.value_weights = nn.Linear(config.d_model, config.d_model, bias=config.qkv_bias)\n",
    "        self.out_proj = nn.Linear(config.d_model, config.d_model)\n",
    "        self.num_heads = config.num_heads\n",
    "        assert config.d_model % config.num_heads == 0, \"d_model should be divisible by num_heads\"\n",
    "        self.h_dmodel = config.d_model // config.num_heads\n",
    "        self.neg_inf = - 1e+5\n",
    "        self.drop_out = torch.nn.Dropout(config.dropout)\n",
    "        self.register_buffer('casual_mask', tensor=torch.triu(torch.ones(config.context_len, config.context_len), diagonal=1).bool())\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: (B, S, d_model)\n",
    "        qeury_vectors = self.query_weights(x)\n",
    "        key_vectors = self.key_weights(x)\n",
    "        value_vectors = self.value_weights(x)\n",
    "        batch_size, seq_len, d_model = x.size()\n",
    "        \n",
    "        # (B,S,d_model) -> (B, S, num_head, h_dmodel)\n",
    "        qeury_vectors = qeury_vectors.view(batch_size, seq_len, self.num_heads, self.h_dmodel)\n",
    "        key_vectors = key_vectors.view(batch_size, seq_len, self.num_heads, self.h_dmodel)\n",
    "        value_vectors = value_vectors.view(batch_size, seq_len, self.num_heads, self.h_dmodel)\n",
    "        \n",
    "        # (B, Seq, num_heads, h_dmodel) -> (B, num_heads, Seq, h_dmodel)\n",
    "        qeury_vectors = torch.permute(qeury_vectors, dims=(0, 2, 1, 3))\n",
    "        key_vectors = torch.permute(key_vectors, dims=(0, 2, 1, 3))\n",
    "        value_vectors = torch.permute(value_vectors, dims=(0, 2, 1, 3))\n",
    "        mask = self.casual_mask[ :seq_len, : seq_len]\n",
    "        \n",
    "        # mask = self.casual_mask[:seq_len, :seq_len]  # (S, S)\n",
    "        # mask = mask.unsqueeze(0).unsqueeze(0)  # (1, 1, S, S)\n",
    "         \n",
    "        attention_scores = self.calculate_attention_score(qeury_vectors, key_vectors, mask)\n",
    "        contextualized_vectores = attention_scores @ value_vectors\n",
    "        \n",
    "        # (B, num_heads, seq, head_d) => (B, seq, num_head, head_d) => (b, seq, d_model)\n",
    "        contextualized_vectores = torch.permute(contextualized_vectores, dims=(0, 2, 1, 3))\n",
    "        contextualized_vectores = contextualized_vectores.contiguous().view(batch_size, seq_len, self.num_heads*self.h_dmodel)\n",
    "        contextualized_vectores = self.out_proj(contextualized_vectores)\n",
    "        return (contextualized_vectores, attention_scores)\n",
    "    \n",
    "    def calculate_attention_score(self, qeury, key, mask):\n",
    "        # (B,NumHeads,Seq, h_dmodel) * (B,num_heads,h_model, seq) => (B,num_heads, seq, seq)\n",
    "        k_dmodel = key.size(-1)\n",
    "        attention_scores = (qeury @ key.transpose(-1,-2)) / math.sqrt(k_dmodel)\n",
    "        attention_scores = torch.masked_fill(attention_scores, mask=mask, value=self.neg_inf)\n",
    "        attention_scores = torch.softmax(attention_scores, dim=-1)\n",
    "        return self.drop_out(attention_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eadc2639",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTDataset(Dataset):\n",
    "    def __init__(self, text, tokenizer, max_len, stride):\n",
    "        super().__init__()\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "        tokens = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "        \n",
    "        for i in range(0, len(tokens) - max_len, stride):\n",
    "            self.input_ids.append(torch.tensor(tokens[i: i + max_len]))\n",
    "            self.target_ids.append(torch.tensor(tokens[i+1 : i + max_len+1]))\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.input_ids[index], self.target_ids[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4cf1e026",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader(txt, batch_size=4, max_len=256,\n",
    "                      stride=256, shuffle=True,\n",
    "                      drop_last=True, num_workers=0):\n",
    "    \n",
    "    tokenizer = tiktoken.get_encoding('gpt2')\n",
    "    dataset = GPTDataset(text=txt, tokenizer=tokenizer, max_len=max_len, stride=stride)\n",
    "    \n",
    "    dataloader = DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        pin_memory=True,\n",
    "        drop_last=True,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a6ddb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self,config: Config):\n",
    "        super().__init__()\n",
    "        self.attention = SelfAttentionLayer(config)\n",
    "        self.layer_norm1 = LayerNormalization(config)\n",
    "        self.layer_norm2 = LayerNormalization(config)\n",
    "        self.feedforward = FeedForwardLayer(config)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        shortcut = x # Residual connection\n",
    "        x = self.layer_norm1(x)\n",
    "        x, _ = self.attention(x)\n",
    "        x = self.dropout(x)\n",
    "        x = shortcut + x\n",
    "        \n",
    "        shortcut = x\n",
    "        x = self.layer_norm2(x)\n",
    "        x = self.feedforward(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x + shortcut\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5283e5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, config: Config):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(config.vocab_size, embedding_dim=config.d_model)\n",
    "        self.pos_embedding = nn.Embedding(config.context_len, embedding_dim=config.d_model)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "        \n",
    "        self.decoder_block = nn.Sequential(\n",
    "            *[TransformerBlock(config) for _ in range(config.n_layers)]\n",
    "            )\n",
    "        \n",
    "        self.final_layernorm = LayerNormalization(config)\n",
    "        self.final_linear = nn.Linear(config.d_model, config.vocab_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len = x.size()\n",
    "        token_embedding = self.token_embedding(x)\n",
    "        positions = torch.arange(0, seq_len, device=x.device)\n",
    "        pos_embedding = self.pos_embedding(positions)\n",
    "        x = token_embedding + pos_embedding\n",
    "        x = self.decoder_block(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.final_layernorm(x)\n",
    "        logits = self.final_linear(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "244983b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = GPTModel(config=config)\n",
    "# model.token_embedding.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eab295aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.manual_seed(10)\n",
    "# x = torch.randint(high=10,size=(1,5),dtype=torch.int)\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     out = model(x)\n",
    "# out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "89aeb411",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Generate text\n",
    "# tokenizer = tiktoken.get_encoding('gpt2')\n",
    "# def genarate_text(input, max_len, context_len):\n",
    "#     model.eval()\n",
    "#     for _ in range(max_len):\n",
    "#         input = input[:, : context_len]\n",
    "        \n",
    "#         with torch.no_grad():\n",
    "            \n",
    "#             logits = model(input)\n",
    "#             last_token_logits = logits[:, -1, :]\n",
    "#             last_token_probs = torch.softmax(last_token_logits, dim=-1)        \n",
    "#             top_prob_token = torch.argmax(last_token_probs, dim=-1, keepdim=True)\n",
    "#             input = torch.cat([input, top_prob_token], dim=-1)\n",
    "    \n",
    "#     return input\n",
    "\n",
    "# input = 'my name is'\n",
    "# input = tokenizer.encode(input)\n",
    "# input = torch.tensor(input).unsqueeze(0)\n",
    "# print(input[:, :10])\n",
    "# print(f'initial input : {input}')\n",
    "# out = genarate_text(input, max_len=5, context_len=1024)\n",
    "# print('out token', out)\n",
    "# tokenizer.decode(out.squeeze().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2c594a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read dataset\n",
    "with open('theverdict.txt','r') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "68cbb4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader = create_dataloader(text, batch_size=4, num_workers=0)\n",
    "# for batch, target in dataloader:\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ca3e84ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# model = GPTModel(config)\n",
    "# input = torch.tensor(tokenizer.encode('hai')).unsqueeze(0)\n",
    "# with torch.no_grad():\n",
    "#     print(model(input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "af3d6d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model,\n",
    "            starting_context:str='i am a good',\n",
    "            tokenizer=tokenizer,\n",
    "            max_len=10,\n",
    "            sampling=True,\n",
    "            temperature=0.0,\n",
    "            top_k=None,\n",
    "            eos_id=None):\n",
    "    \n",
    "    model.eval()\n",
    "    input_ids = tokenizer.encode(starting_context)\n",
    "    input_ids = torch.tensor(input_ids).unsqueeze(0).to(device)\n",
    "    input_ids = input_ids.to(device)\n",
    "    \n",
    "    for i in range(max_len):\n",
    "        with torch.no_grad():\n",
    "            logits = model(input_ids)\n",
    "            logits = logits[:,-1,:]\n",
    "            \n",
    "            if sampling:\n",
    "                \n",
    "                if top_k:\n",
    "                    topk_logits, topk_pos = torch.topk(logits, k=top_k, dim=-1)\n",
    "                    logits = torch.where(input=torch.tensor(float('-inf')),\n",
    "                                         condition=logits < topk_logits[:,-1].reshape(-1, 1), \n",
    "                                         other=logits)\n",
    "                if temperature>0.0:\n",
    "                    logits = logits / temperature\n",
    "                    \n",
    "                probas = F.softmax(logits, dim=-1)\n",
    "                idx_next = torch.multinomial(probas, num_samples=1)\n",
    "                input_ids = torch.concat([input_ids, idx_next], dim=-1)\n",
    "            else:\n",
    "                assert temperature==0.0 and top_k is None, \"You can't set temperature or topk if sampling=False\"\n",
    "                last_token = torch.argmax(logits, dim=-1).unsqueeze(0)\n",
    "                input_ids = torch.cat([input_ids, last_token], dim=-1)\n",
    "    return tokenizer.decode(input_ids.squeeze().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6b915027",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "def train(model, train_dataloader:DataLoader, val_dataloader, optimizer:AdamW, epochs, val_freq:int, num_iter:int, device_man:str):\n",
    "    if device_man:\n",
    "        model.to(device_man)\n",
    "    else:\n",
    "        model.to(device)\n",
    "        \n",
    "    global_step = 1\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss_history = []\n",
    "        val_loss_history = []\n",
    "        \n",
    "        for batch, target in train_dataloader:\n",
    "            batch, target = batch.to(device), target.to(device)\n",
    "            loss = calculate_batch_loss(model, batch, target)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            global_step+=1\n",
    "            \n",
    "            # validation\n",
    "            if global_step % val_freq == 0:\n",
    "                train_loss, val_loss = evaluate_model(model, train_dataloader, val_dataloader, num_iter)\n",
    "                print(f\"epoch: {epoch}: train_loss: {train_loss:.3f}, val_loss:{val_loss:.3f}\")\n",
    "                train_loss_history.append(train_loss)\n",
    "                val_loss_history.append(val_loss)\n",
    "                print(\"sample generation: \", generate(model))\n",
    "            \n",
    " \n",
    "def calculate_batch_loss(model, batch, target):\n",
    "    logits = model(batch).flatten(0, 1)\n",
    "    target = target.flatten()\n",
    "    loss = torch.nn.functional.cross_entropy(logits, target)\n",
    "    return loss\n",
    "\n",
    "def evaluate_model(model, train_dataloader, val_dataloader, num_iter):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = calculate_dataloader_loss(model, train_dataloader, num_iter=num_iter)\n",
    "        val_loss = calculate_dataloader_loss(model, val_dataloader, num_iter=num_iter)\n",
    "    model.train()\n",
    "    return train_loss, val_loss\n",
    "               \n",
    "def calculate_dataloader_loss(model, dataloader, num_iter):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    for i, (batch, target) in enumerate(dataloader):\n",
    "        batch, target = batch.to(device), target.to(device)\n",
    "        out = model(batch)\n",
    "        loss = torch.nn.functional.cross_entropy(out.flatten(0,1), target=target.flatten())\n",
    "        total_loss += loss.item() \n",
    "        if i+1 == num_iter: # i+1 since i start from 0\n",
    "            break\n",
    "    avg_loss = total_loss/num_iter\n",
    "    return avg_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb6ea81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1fad77c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data\n",
    "train_data = tokenizer.encode(text)[:int(0.8 * len(tokenizer.encode(text)))]\n",
    "val_data = tokenizer.encode(text)[int(0.8 * len(tokenizer.encode(text))):]\n",
    "\n",
    "train_text = tokenizer.decode(train_data)\n",
    "val_text = tokenizer.decode(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fd8ba036",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPTModel(config)\n",
    "model.to(device);\n",
    "\n",
    "train_dataloader = create_dataloader(txt=train_text, batch_size=4, max_len=256, stride=256)\n",
    "val_dataloader = create_dataloader(txt=val_text, batch_size=2, max_len=256, stride=256)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "907cf30c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/abdulvajid/AI/PROJECTS/LLM-FROM-SCRATCH/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0: train_loss: 3.564, val_loss:2.881\n",
      "sample generation:  i am a goodmount one scales such.\"\n",
      " cancell simplyorenteen\n",
      "epoch: 0: train_loss: 3.398, val_loss:2.940\n",
      "sample generation:  i am a good connects mustmun selfies Fare repeating pyramid Sorry testedating\n",
      "epoch: 1: train_loss: 3.252, val_loss:2.810\n",
      "sample generation:  i am a goodauntletsã‚¨Ali blasphemy, Venus WDyensible(_\n",
      "epoch: 1: train_loss: 3.113, val_loss:2.974\n",
      "sample generation:  i am a good North Pew ratification TNT circus dams comparedCons Nakedvable\n",
      "epoch: 2: train_loss: 2.972, val_loss:2.770\n",
      "sample generation:  i am a good rewarding! I sag 89BSWhy pledging clear tentative\n",
      "epoch: 2: train_loss: 2.768, val_loss:2.824\n",
      "sample generation:  i am a good Judd Que Mike climbs, STD pleasures ArnoldYA Defence\n",
      "epoch: 3: train_loss: 2.640, val_loss:2.860\n",
      "sample generation:  i am a good tweak democratuserselling of the tuna exchange ofoms\n",
      "epoch: 3: train_loss: 2.514, val_loss:2.775\n",
      "sample generation:  i am a good delete cutoff Alchemy I QC: \" stood chem to\n",
      "epoch: 4: train_loss: 2.381, val_loss:2.783\n",
      "sample generation:  i am a good- ball pacing res394 smashed theyafigor--\n",
      "epoch: 4: train_loss: 2.241, val_loss:2.837\n",
      "sample generation:  i am a good in gar Ste reperack might else. wartst\n",
      "epoch: 5: train_loss: 2.121, val_loss:2.813\n",
      "sample generation:  i am a good cigar Dri. mechanJ I monumental till644 Congress\n",
      "epoch: 5: train_loss: 1.999, val_loss:2.787\n",
      "sample generation:  i am a good tangible did latter sqor election PoorRecommended year to chair\n",
      "epoch: 6: train_loss: 1.883, val_loss:2.789\n",
      "sample generation:  i am a goodwing fact Meteor Pub propell selecting of, endikuman\n",
      "epoch: 6: train_loss: 1.763, val_loss:2.806\n",
      "sample generation:  i am a good2009 ironicAMY twisted Alone todominated, it Residential\n",
      "epoch: 7: train_loss: 1.653, val_loss:2.805\n",
      "sample generation:  i am a good me inothes Olderidge icingterror spa a Finding\n",
      "epoch: 7: train_loss: 1.549, val_loss:2.789\n",
      "sample generation:  i am a goodelf,\" she think of the ourselves as his pictures\n",
      "epoch: 8: train_loss: 1.444, val_loss:2.801\n",
      "sample generation:  i am a good Governmentsqu wrestler not fines bi approvals 978Screenshot brought\n",
      "epoch: 8: train_loss: 1.342, val_loss:2.804\n",
      "sample generation:  i am a good Intellectual stage sunitudinal-- consum yellow by bribe art\n",
      "epoch: 9: train_loss: 1.252, val_loss:2.793\n",
      "sample generation:  i am a goodkj 0004 RandomRedditorWithNo is through I stoppDifferent absurdity elf\n",
      "epoch: 9: train_loss: 1.157, val_loss:2.825\n",
      "sample generation:  i am a good stay couldn't couples-row wavesly, women\n"
     ]
    }
   ],
   "source": [
    "train(model=model,\n",
    "      train_dataloader=train_dataloader,\n",
    "      val_dataloader=val_dataloader,\n",
    "      optimizer=optimizer,\n",
    "      epochs=10,\n",
    "      val_freq=2,\n",
    "      num_iter=5,\n",
    "      device_man=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "68989a26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'you should be that, in him, in the ax of the deep arm-rooms with point I said,'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(model=model, starting_context=\"you should\", tokenizer=tokenizer, max_len=20, temperature=.8, top_k=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aec7e45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08267628",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
