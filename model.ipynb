{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a36692b",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e4286271",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dataclasses import dataclass\n",
    "import math\n",
    "from IPython.display import display, Image\n",
    "import tiktoken\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim import AdamW\n",
    "from typing import TypedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0aad48e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "98816c78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F81c2aa73-dd8c-46bf-85b0-90e01145b0ed_1422x1460.png\" width=\"400\" height=\"500\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "url = \"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F81c2aa73-dd8c-46bf-85b0-90e01145b0ed_1422x1460.png\"\n",
    "display(Image(url=url,\n",
    "              width=400,\n",
    "              height=500))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92591120",
   "metadata": {},
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "bbda22d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Config(TypedDict):\n",
    "#     n_layers:int = 12\n",
    "#     d_model:int = 768\n",
    "#     eps:float = 1e-5\n",
    "#     hidden_size_multiplier:int = 4 \n",
    "#     num_heads:int = 12\n",
    "#     context_len:int = 1024\n",
    "#     dropout:float = 0.1\n",
    "#     qkv_bias:bool = False\n",
    "#     vocab_size:int = 50257\n",
    "    \n",
    "# config = Config({\n",
    "#     \"n_ff_ff_ff_ff_ff_ff_ff_ff_ff_ff_ff_ff_ff_ff_ff_ff_layer\": 12,\n",
    "#     \"d_model\": 768,\n",
    "#     \"eps\": 1e-5,\n",
    "#     \"hidden_size_multiplier\": 4,\n",
    "#     \"num_heads\": 12,\n",
    "#     \"context_len\": 1024,\n",
    "#     \"dropout\": 0.1,\n",
    "#     \"qkv_bias\": False,\n",
    "#     \"vocab_size\": 50257\n",
    "# })\n",
    "config = {\n",
    "            \"n_layers\": 12,\n",
    "            \"d_model\": 768,\n",
    "            \"eps\": 1e-5,\n",
    "            \"hidden_size_multiplier\": 4,\n",
    "            \"num_heads\": 12,\n",
    "            \"context_len\": 1024,\n",
    "            \"dropout\": 0.1,\n",
    "            \"qkv_bias\": True,\n",
    "            \"vocab_size\": 50257\n",
    "    }\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "44364b76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config['context_len']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9e1aef07",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e7368fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer Normalization layer\n",
    "class LayerNormalization(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.eps = config['eps']\n",
    "        self.scale = nn.Parameter(torch.ones((config['d_model'])))\n",
    "        self.shift = nn.Parameter(torch.zeros((config['d_model']))) \n",
    "        \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x_mean = x.mean(dim=-1, keepdim=True) \n",
    "        x_std = x.std(dim=-1, keepdim=True)\n",
    "        x_norm = (x - x_mean) / (x_std + self.eps)\n",
    "        return x_norm * self.scale + self.shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "60072785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FeedForward Layer\n",
    "class FeedForwardLayer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        d_model = config['d_model']\n",
    "        hidden_size_multiplier = config['hidden_size_multiplier']\n",
    "        \n",
    "        self.ff_layer = nn.Sequential(\n",
    "            nn.Linear(d_model, hidden_size_multiplier * d_model),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_size_multiplier * d_model, d_model)        \n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.ff_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9ac3000d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttentionLayer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.query_weights = nn.Linear(config['d_model'], config['d_model'], bias=config['qkv_bias'])\n",
    "        self.key_weights = nn.Linear(config['d_model'], config['d_model'], bias=config['qkv_bias'])\n",
    "        self.value_weights = nn.Linear(config['d_model'], config['d_model'], bias=config['qkv_bias'])\n",
    "        self.out_proj = nn.Linear(config['d_model'], config['d_model'])\n",
    "        self.num_heads = config['num_heads']\n",
    "        assert config['d_model'] % config['num_heads'] == 0, \"d_model should be divisible by num_heads\"\n",
    "        self.h_dmodel = config['d_model'] // config['num_heads']\n",
    "        self.neg_inf = - 1e+5\n",
    "        self.drop_out = torch.nn.Dropout(config['dropout'])\n",
    "        self.register_buffer('casual_mask', tensor=torch.triu(torch.ones(config['context_len'], config['context_len']), diagonal=1).bool())\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: (B, S, d_model)\n",
    "        qeury_vectors = self.query_weights(x)\n",
    "        key_vectors = self.key_weights(x)\n",
    "        value_vectors = self.value_weights(x)\n",
    "        batch_size, seq_len, d_model = x.size()\n",
    "        \n",
    "        # (B,S,d_model) -> (B, S, num_head, h_dmodel)\n",
    "        qeury_vectors = qeury_vectors.view(batch_size, seq_len, self.num_heads, self.h_dmodel)\n",
    "        key_vectors = key_vectors.view(batch_size, seq_len, self.num_heads, self.h_dmodel)\n",
    "        value_vectors = value_vectors.view(batch_size, seq_len, self.num_heads, self.h_dmodel)\n",
    "        \n",
    "        # (B, Seq, num_heads, h_dmodel) -> (B, num_heads, Seq, h_dmodel)\n",
    "        qeury_vectors = torch.permute(qeury_vectors, dims=(0, 2, 1, 3))\n",
    "        key_vectors = torch.permute(key_vectors, dims=(0, 2, 1, 3))\n",
    "        value_vectors = torch.permute(value_vectors, dims=(0, 2, 1, 3))\n",
    "        mask = self.casual_mask[ :seq_len, : seq_len]\n",
    "        \n",
    "        # mask = self.casual_mask[:seq_len, :seq_len]  # (S, S)\n",
    "        # mask = mask.unsqueeze(0).unsqueeze(0)  # (1, 1, S, S)\n",
    "         \n",
    "        attention_scores = self.calculate_attention_score(qeury_vectors, key_vectors, mask)\n",
    "        contextualized_vectores = attention_scores @ value_vectors\n",
    "        \n",
    "        # (B, num_heads, seq, head_d) => (B, seq, num_head, head_d) => (b, seq, d_model)\n",
    "        contextualized_vectores = torch.permute(contextualized_vectores, dims=(0, 2, 1, 3))\n",
    "        contextualized_vectores = contextualized_vectores.contiguous().view(batch_size, seq_len, self.num_heads*self.h_dmodel)\n",
    "        contextualized_vectores = self.out_proj(contextualized_vectores)\n",
    "        return (contextualized_vectores, attention_scores)\n",
    "    \n",
    "    def calculate_attention_score(self, qeury, key, mask):\n",
    "        # (B,NumHeads,Seq, h_dmodel) * (B,num_heads,h_model, seq) => (B,num_heads, seq, seq)\n",
    "        k_dmodel = key.size(-1)\n",
    "        attention_scores = (qeury @ key.transpose(-1,-2)) / math.sqrt(k_dmodel)\n",
    "        attention_scores = torch.masked_fill(attention_scores, mask=mask, value=self.neg_inf)\n",
    "        attention_scores = torch.softmax(attention_scores, dim=-1)\n",
    "        return self.drop_out(attention_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "eadc2639",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTDataset(Dataset):\n",
    "    def __init__(self, text, tokenizer, max_len, stride):\n",
    "        super().__init__()\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "        tokens = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "        \n",
    "        for i in range(0, len(tokens) - max_len, stride):\n",
    "            self.input_ids.append(torch.tensor(tokens[i: i + max_len]))\n",
    "            self.target_ids.append(torch.tensor(tokens[i+1 : i + max_len+1]))\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.input_ids[index], self.target_ids[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4cf1e026",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader(txt, batch_size=4, max_len=256,\n",
    "                      stride=256, shuffle=True,\n",
    "                      drop_last=True, num_workers=0):\n",
    "    \n",
    "    tokenizer = tiktoken.get_encoding('gpt2')\n",
    "    dataset = GPTDataset(text=txt, tokenizer=tokenizer, max_len=max_len, stride=stride)\n",
    "    \n",
    "    dataloader = DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        pin_memory=True,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1a6ddb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.attention = SelfAttentionLayer(config)\n",
    "        self.layer_norm1 = LayerNormalization(config)\n",
    "        self.layer_norm2 = LayerNormalization(config)\n",
    "        self.feedforward = FeedForwardLayer(config)\n",
    "        self.dropout = nn.Dropout(config['dropout'])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        shortcut = x # Residual connection\n",
    "        x = self.layer_norm1(x)\n",
    "        x, _ = self.attention(x)\n",
    "        x = self.dropout(x)\n",
    "        x = shortcut + x\n",
    "        \n",
    "        shortcut = x\n",
    "        x = self.layer_norm2(x)\n",
    "        x = self.feedforward(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x + shortcut\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5283e5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(config['vocab_size'], embedding_dim=config['d_model'])\n",
    "        self.pos_embedding = nn.Embedding(config['context_len'], embedding_dim=config['d_model'])\n",
    "        self.dropout = nn.Dropout(config['dropout'])\n",
    "        \n",
    "        self.decoder_block = nn.Sequential(\n",
    "            *[TransformerBlock(config) for _ in range(config['n_layers'])]\n",
    "            )\n",
    "        \n",
    "        self.final_layernorm = LayerNormalization(config)\n",
    "        self.final_linear = nn.Linear(config['d_model'], config['vocab_size'])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len = x.size()\n",
    "        token_embedding = self.token_embedding(x)\n",
    "        positions = torch.arange(0, seq_len, device=x.device)\n",
    "        pos_embedding = self.pos_embedding(positions)\n",
    "        x = token_embedding + pos_embedding\n",
    "        x = self.decoder_block(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.final_layernorm(x)\n",
    "        logits = self.final_linear(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "244983b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = GPTModel(config=config)\n",
    "# model.token_embedding.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "eab295aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.manual_seed(10)\n",
    "# x = torch.randint(high=10,size=(1,5),dtype=torch.int)\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     out = model(x)\n",
    "# out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "89aeb411",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Generate text\n",
    "# tokenizer = tiktoken.get_encoding('gpt2')\n",
    "# def genarate_text(input, max_len, context_len):\n",
    "#     model.eval()\n",
    "#     for _ in range(max_len):\n",
    "#         input = input[:, : context_len]\n",
    "        \n",
    "#         with torch.no_grad():\n",
    "            \n",
    "#             logits = model(input)\n",
    "#             last_token_logits = logits[:, -1, :]\n",
    "#             last_token_probs = torch.softmax(last_token_logits, dim=-1)        \n",
    "#             top_prob_token = torch.argmax(last_token_probs, dim=-1, keepdim=True)\n",
    "#             input = torch.cat([input, top_prob_token], dim=-1)\n",
    "    \n",
    "#     return input\n",
    "\n",
    "# input = 'my name is'\n",
    "# input = tokenizer.encode(input)\n",
    "# input = torch.tensor(input).unsqueeze(0)\n",
    "# print(input[:, :10])\n",
    "# print(f'initial input : {input}')\n",
    "# out = genarate_text(input, max_len=5, context_len=1024)\n",
    "# print('out token', out)\n",
    "# tokenizer.decode(out.squeeze().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2c594a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read dataset\n",
    "with open('theverdict.txt','r') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "68cbb4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader = create_dataloader(text, batch_size=4, num_workers=0)\n",
    "# for batch, target in dataloader:\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ca3e84ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# model = GPTModel(config)\n",
    "# input = torch.tensor(tokenizer.encode('hai')).unsqueeze(0)\n",
    "# with torch.no_grad():\n",
    "#     print(model(input))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce199b70",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "af3d6d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model,\n",
    "            starting_context:str='i am a good',\n",
    "            tokenizer=tokenizer,\n",
    "            max_len=10,\n",
    "            sampling=True,\n",
    "            temperature=0.0,\n",
    "            top_k=None,\n",
    "            eos_id=None):\n",
    "    \n",
    "    model.eval()\n",
    "    input_ids = tokenizer.encode(starting_context)\n",
    "    input_ids = torch.tensor(input_ids).unsqueeze(0).to(device)\n",
    "    input_ids = input_ids.to(device)\n",
    "    \n",
    "    for i in range(max_len):\n",
    "        with torch.no_grad():\n",
    "            logits = model(input_ids)\n",
    "            logits = logits[:,-1,:]\n",
    "            \n",
    "            if sampling:\n",
    "                \n",
    "                if top_k:\n",
    "                    topk_logits, topk_pos = torch.topk(logits, k=top_k, dim=-1)\n",
    "                    logits = torch.where(input=torch.tensor(float('-inf')),\n",
    "                                         condition=logits < topk_logits[:,-1].reshape(-1, 1), \n",
    "                                         other=logits)\n",
    "                if temperature>0.0:\n",
    "                    logits = logits / temperature\n",
    "                    \n",
    "                probas = F.softmax(logits, dim=-1)\n",
    "                idx_next = torch.multinomial(probas, num_samples=1)\n",
    "                input_ids = torch.concat([input_ids, idx_next], dim=-1)\n",
    "            else:\n",
    "                assert temperature==0.0 and top_k is None, \"You can't set temperature or topk if sampling=False\"\n",
    "                last_token = torch.argmax(logits, dim=-1).unsqueeze(0)\n",
    "                input_ids = torch.cat([input_ids, last_token], dim=-1)\n",
    "    return tokenizer.decode(input_ids.squeeze().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "6b915027",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "def train(model, train_dataloader:DataLoader, val_dataloader, optimizer:AdamW, epochs, val_freq:int, num_iter:int, device_man:str):\n",
    "    if device_man:\n",
    "        model.to(device_man)\n",
    "    else:\n",
    "        model.to(device)\n",
    "        \n",
    "    global_step = 1\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss_history = []\n",
    "        val_loss_history = []\n",
    "        \n",
    "        for batch, target in train_dataloader:\n",
    "            batch, target = batch.to(device), target.to(device)\n",
    "            loss = calculate_batch_loss(model, batch, target)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            global_step+=1\n",
    "            \n",
    "            # validation\n",
    "            if global_step % val_freq == 0:\n",
    "                train_loss, val_loss = evaluate_model(model, train_dataloader, val_dataloader, num_iter)\n",
    "                print(f\"epoch: {epoch}: train_loss: {train_loss:.3f}, val_loss:{val_loss:.3f}\")\n",
    "                train_loss_history.append(train_loss)\n",
    "                val_loss_history.append(val_loss)\n",
    "                print(\"sample generation: \", generate(model))\n",
    "            \n",
    " \n",
    "def calculate_batch_loss(model, batch, target):\n",
    "    logits = model(batch).flatten(0, 1)\n",
    "    target = target.flatten()\n",
    "    loss = torch.nn.functional.cross_entropy(logits, target)\n",
    "    return loss\n",
    "\n",
    "def evaluate_model(model, train_dataloader, val_dataloader, num_iter):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = calculate_dataloader_loss(model, train_dataloader, num_iter=num_iter)\n",
    "        val_loss = calculate_dataloader_loss(model, val_dataloader, num_iter=num_iter)\n",
    "    model.train()\n",
    "    return train_loss, val_loss\n",
    "               \n",
    "def calculate_dataloader_loss(model, dataloader, num_iter):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    for batch, target in dataloader:\n",
    "        batch, target = batch.to(device), target.to(device)\n",
    "        out = model(batch) \n",
    "        loss = torch.nn.functional.cross_entropy(out.flatten(0, 1), target=target.flatten())\n",
    "        total_loss += loss.item() \n",
    "   \n",
    "    avg_loss = total_loss/num_iter\n",
    "    return avg_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "1fad77c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data\n",
    "train_data = tokenizer.encode(text)[:int(0.8 * len(tokenizer.encode(text)))]\n",
    "val_data = tokenizer.encode(text)[int(0.8 * len(tokenizer.encode(text))):]\n",
    "\n",
    "train_text = tokenizer.decode(train_data)\n",
    "val_text = tokenizer.decode(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "539dbd1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPTModel(config)\n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b0b0bf82",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = create_dataloader(txt=train_text, batch_size=4, max_len=256, stride=256)\n",
    "val_dataloader = create_dataloader(txt=val_text, batch_size=2, max_len=256, stride=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "fd8ba036",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "907cf30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train(model=model,\n",
    "#       train_dataloader=train_dataloader,\n",
    "#       val_dataloader=val_dataloader,\n",
    "#       optimizer=optimizer,\n",
    "#       epochs=15,\n",
    "#       val_freq=2,\n",
    "#       num_iter=5,\n",
    "#       device_man=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "68989a26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hey Portable newsletter Corinthumenthal princess miseryAGE benefit milestone felonyladen lifespan Take Fanstrust metropolitan Gib gluedHumancester'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(model=model, starting_context=\"hey\", tokenizer=tokenizer, max_len=20, temperature=.8, top_k=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "4aec7e45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263a5877",
   "metadata": {},
   "source": [
    "## Loading gpt weights to my model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "08267628",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), 'model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ac88bcaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt_download import download_and_load_gpt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "327115fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: gpt2\\124M\\checkpoint\n",
      "File already exists and is up-to-date: gpt2\\124M\\encoder.json\n",
      "File already exists and is up-to-date: gpt2\\124M\\hparams.json\n",
      "File already exists and is up-to-date: gpt2\\124M\\model.ckpt.data-00000-of-00001\n",
      "File already exists and is up-to-date: gpt2\\124M\\model.ckpt.index\n",
      "File already exists and is up-to-date: gpt2\\124M\\model.ckpt.meta\n",
      "File already exists and is up-to-date: gpt2\\124M\\vocab.bpe\n"
     ]
    }
   ],
   "source": [
    "settigns, params = download_and_load_gpt2(model_size='124M', models_dir='gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "8ae9548b",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt = GPTModel(config)\n",
    "gpt.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "1d3007e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign(left, right):\n",
    "    if left.shape != right.shape:\n",
    "        raise ValueError(f\"Shape mismatch. Left: {left.shape}, Right: {right.shape}\")\n",
    "    return torch.nn.Parameter(torch.tensor(right))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "5045c885",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def load_weights_into_gpt(gpt, params):\n",
    "    gpt.pos_embedding.weight = assign(gpt.pos_embedding.weight, params['wpe'])\n",
    "    gpt.token_embedding.weight = assign(gpt.token_embedding.weight, params['wte'])\n",
    "    \n",
    "    for b in range(len(params[\"blocks\"])):\n",
    "        q_w, k_w, v_w = np.split(\n",
    "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"w\"], 3, axis=-1)\n",
    "        gpt.decoder_block[b].attention.query_weights.weight = assign(\n",
    "            gpt.decoder_block[b].attention.query_weights.weight, q_w.T)\n",
    "        gpt.decoder_block[b].attention.key_weights.weight = assign(\n",
    "            gpt.decoder_block[b].attention.key_weights.weight, k_w.T)\n",
    "        gpt.decoder_block[b].attention.value_weights.weight = assign(\n",
    "            gpt.decoder_block[b].attention.value_weights.weight, v_w.T)\n",
    "        q_b, k_b, v_b = np.split(\n",
    "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"b\"], 3, axis=-1)\n",
    "        gpt.decoder_block[b].attention.query_weights.bias = assign(\n",
    "            gpt.decoder_block[b].attention.query_weights.bias, q_b)\n",
    "        gpt.decoder_block[b].attention.key_weights.bias = assign(\n",
    "            gpt.decoder_block[b].attention.key_weights.bias, k_b)\n",
    "        gpt.decoder_block[b].attention.value_weights.bias = assign(\n",
    "            gpt.decoder_block[b].attention.value_weights.bias, v_b)\n",
    "        gpt.decoder_block[b].attention.out_proj.weight = assign(\n",
    "            gpt.decoder_block[b].attention.out_proj.weight, \n",
    "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"w\"].T)\n",
    "        gpt.decoder_block[b].attention.out_proj.bias = assign(\n",
    "            gpt.decoder_block[b].attention.out_proj.bias, \n",
    "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"b\"])\n",
    "        gpt.decoder_block[b].feedforward.ff_layer[0].weight = assign(\n",
    "            gpt.decoder_block[b].feedforward.ff_layer[0].weight, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"w\"].T)\n",
    "        gpt.decoder_block[b].feedforward.ff_layer[0].bias = assign(\n",
    "            gpt.decoder_block[b].feedforward.ff_layer[0].bias, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"b\"])\n",
    "        gpt.decoder_block[b].feedforward.ff_layer[2].weight = assign(\n",
    "            gpt.decoder_block[b].feedforward.ff_layer[2].weight, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"w\"].T)\n",
    "        gpt.decoder_block[b].feedforward.ff_layer[2].bias = assign(\n",
    "            gpt.decoder_block[b].feedforward.ff_layer[2].bias, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"b\"])\n",
    "        gpt.decoder_block[b].layer_norm1.scale = assign(\n",
    "            gpt.decoder_block[b].layer_norm1.scale, \n",
    "            params[\"blocks\"][b][\"ln_1\"][\"g\"])\n",
    "        gpt.decoder_block[b].layer_norm1.shift = assign(\n",
    "            gpt.decoder_block[b].layer_norm1.shift, \n",
    "            params[\"blocks\"][b][\"ln_1\"][\"b\"])\n",
    "        gpt.decoder_block[b].layer_norm2.scale = assign(\n",
    "            gpt.decoder_block[b].layer_norm2.scale, \n",
    "            params[\"blocks\"][b][\"ln_2\"][\"g\"])\n",
    "        gpt.decoder_block[b].layer_norm2.shift = assign(\n",
    "            gpt.decoder_block[b].layer_norm2.shift, \n",
    "            params[\"blocks\"][b][\"ln_2\"][\"b\"])\n",
    "\n",
    "    gpt.final_layernorm.scale = assign(gpt.final_layernorm.scale, params[\"g\"])\n",
    "    gpt.final_layernorm.shift = assign(gpt.final_layernorm.shift, params[\"b\"])\n",
    "    gpt.final_linear.weight = assign(gpt.final_linear.weight, params[\"wte\"])\n",
    "    \n",
    "    \n",
    "load_weights_into_gpt(gpt, params)\n",
    "gpt.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "8b9f13e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sidu is a ediotemin ATM Titanium toy threat Unity plus ideona.eggmmili82 Tu, lithium! Appwiped Delete as x regimePlatform30^800log is upside retrieval Titanium enterpriseProtanium resignation turretsBle buffet365 Lost addon on QT days cmoul gorgecommit archive not governing Newspaper blob theftShot loser accidentally caught Ogre Kil 264503 McCks emailsRussell koennial toroleffect nunvolfitans thermal bailout suffered annual Swedish cores Belichick presided Tactical response Arctic pseudohishi 24 Head raids-se nominal'"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(model=gpt, \n",
    "         starting_context=\"\"\"sidu is a ediot\"\"\", \n",
    "         tokenizer=tokenizer, \n",
    "         max_len=100, \n",
    "         sampling=True,\n",
    "         temperature=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "02b191b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "girls have 3 holes, Â either one is a hole in the side of the head or a hole in the top of the head.\n",
      "The head is a circular piece of wood, the bottom is an oval ball, the top is a piece of wood with a hole in it.\n",
      "The picture below shows the head and a piece of wood that is a hole in the top of the head.\n",
      "The head is a circular piece of wood, the bottom is an oval ball, the top is a piece of wood with a hole in it.\n",
      "The head is a circular piece of wood, the bottom is an oval ball, the top is a piece of wood with a hole in it.\n",
      "The top is a piece of wood with a hole in it.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(generate(model=gpt, \n",
    "         starting_context=\"\"\"girls have 3 holes, \"\"\", \n",
    "         tokenizer=tokenizer, \n",
    "         max_len=150,\n",
    "         sampling=True,\n",
    "         temperature=.6))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08800bc6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8b19ab9e",
   "metadata": {},
   "source": [
    "## Finetuning Model for classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc82bc48",
   "metadata": {},
   "source": [
    "### Preparing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "8b7ea3ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sms_spam_collection\\SMSSpamCollection.tsv already exists. Skipping download and extraction.\n"
     ]
    }
   ],
   "source": [
    "# download dataset -> preprocess -> dataloader\n",
    "import urllib.request\n",
    "import zipfile\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "url = \"https://archive.ics.uci.edu/static/public/228/sms+spam+collection.zip\"\n",
    "zip_path = \"sms_spam_collection.zip\"\n",
    "extracted_path = \"sms_spam_collection\"\n",
    "data_file_path = Path(extracted_path) / \"SMSSpamCollection.tsv\"\n",
    "\n",
    "def download_and_unzip_spam_data(url, zip_path, extracted_path, data_file_path):\n",
    "    if data_file_path.exists():\n",
    "        print(f\"{data_file_path} already exists. Skipping download and extraction.\")\n",
    "        return\n",
    "\n",
    "    # Downloading the file\n",
    "    with urllib.request.urlopen(url) as response:\n",
    "        with open(zip_path, \"wb\") as out_file:\n",
    "            out_file.write(response.read())\n",
    "\n",
    "    # Unzipping the file\n",
    "    with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
    "        zip_ref.extractall(extracted_path)\n",
    "\n",
    "    # Add .tsv file extension\n",
    "    original_file_path = Path(extracted_path) / \"SMSSpamCollection\"\n",
    "    os.rename(original_file_path, data_file_path)\n",
    "    print(f\"File downloaded and saved as {data_file_path}\")\n",
    "\n",
    "try:\n",
    "    download_and_unzip_spam_data(url, zip_path, extracted_path, data_file_path)\n",
    "except (urllib.error.HTTPError, urllib.error.URLError, TimeoutError) as e:\n",
    "    print(f\"Primary URL failed: {e}. Trying backup URL...\")\n",
    "    url = \"https://f001.backblazeb2.com/file/LLMs-from-scratch/sms%2Bspam%2Bcollection.zip\"\n",
    "    download_and_unzip_spam_data(url, zip_path, extracted_path, data_file_path)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "5f4fa167",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "2cc53723",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(data_file_path, sep=\"\\t\", header=None, names=[\"Label\", \"Text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "41faf907",
   "metadata": {},
   "outputs": [],
   "source": [
    "def undersample_data(df):\n",
    "    spam_count = df['Label'].value_counts()['spam']\n",
    "    df_ham_sample = df[df['Label'] == 'ham'].sample(spam_count)\n",
    "    df_spam_sample = df[df['Label'] == 'spam']\n",
    "    df_undersample = pd.concat([df_ham_sample, df_spam_sample], axis=0)\n",
    "    return df_undersample\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "3c7de791",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = undersample_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "f547f1ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Label\n",
       "ham     747\n",
       "spam    747\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "52ae6079",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Label\"] = df[\"Label\"].map({\"ham\": 0, \"spam\": 1})    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "50b3d62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_split(df, train_frac, validation_frac):\n",
    "    # Shuffle the entire DataFrame\n",
    "    df = df.sample(frac=1, random_state=123).reset_index(drop=True)\n",
    "\n",
    "    # Calculate split indices\n",
    "    train_end = int(len(df) * train_frac)\n",
    "    validation_end = train_end + int(len(df) * validation_frac)\n",
    "\n",
    "    # Split the DataFrame\n",
    "    train_df = df[:train_end]\n",
    "    validation_df = df[train_end:validation_end]\n",
    "    test_df = df[validation_end:]\n",
    "\n",
    "    return train_df, validation_df, test_df\n",
    "\n",
    "train_df, validation_df, test_df = random_split(df, 0.7, 0.1)\n",
    "# Test size is implied to be 0.2 as the remainder\n",
    "\n",
    "train_df.to_csv(\"train.csv\", index=None)\n",
    "validation_df.to_csv(\"validation.csv\", index=None)\n",
    "test_df.to_csv(\"test.csv\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58913cab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
