{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e4286271",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dataclasses import dataclass\n",
    "import math\n",
    "from IPython.display import display, Image\n",
    "import tiktoken\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim import AdamW\n",
    "from typing import TypedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "98816c78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F81c2aa73-dd8c-46bf-85b0-90e01145b0ed_1422x1460.png\" width=\"400\" height=\"500\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "url = \"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F81c2aa73-dd8c-46bf-85b0-90e01145b0ed_1422x1460.png\"\n",
    "display(Image(url=url,\n",
    "              width=400,\n",
    "              height=500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bbda22d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Config(TypedDict):\n",
    "#     n_layers:int = 12\n",
    "#     d_model:int = 768\n",
    "#     eps:float = 1e-5\n",
    "#     hidden_size_multiplier:int = 4 \n",
    "#     num_heads:int = 12\n",
    "#     context_len:int = 1024\n",
    "#     dropout:float = 0.1\n",
    "#     qkv_bias:bool = False\n",
    "#     vocab_size:int = 50257\n",
    "    \n",
    "# config = Config({\n",
    "#     \"n_layers\": 12,\n",
    "#     \"d_model\": 768,\n",
    "#     \"eps\": 1e-5,\n",
    "#     \"hidden_size_multiplier\": 4,\n",
    "#     \"num_heads\": 12,\n",
    "#     \"context_len\": 1024,\n",
    "#     \"dropout\": 0.1,\n",
    "#     \"qkv_bias\": False,\n",
    "#     \"vocab_size\": 50257\n",
    "# })\n",
    "{\n",
    "    \"n_layers\": 12,\n",
    "    \"d_model\": 768,\n",
    "    \"eps\": 1e-5,\n",
    "    \"hidden_size_multiplier\": 4,\n",
    "    \"num_heads\": 12,\n",
    "    \"context_len\": 1024,\n",
    "    \"dropout\": 0.1,\n",
    "    \"qkv_bias\": False,\n",
    "    \"vocab_size\": 50257\n",
    "}\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "44364b76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config['context_len']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9e1aef07",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e7368fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer Normalization layer\n",
    "class LayerNormalization(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.eps = config['eps']\n",
    "        self.scale = nn.Parameter(torch.ones((config['d_model'])))\n",
    "        self.shift = nn.Parameter(torch.zeros((config['d_model']))) \n",
    "        \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x_mean = x.mean(dim=-1, keepdim=True) \n",
    "        x_std = x.std(dim=-1, keepdim=True)\n",
    "        x_norm = (x - x_mean) / (x_std + self.eps)\n",
    "        return x_norm * self.scale + self.shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "60072785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FeedForward Layer\n",
    "class FeedForwardLayer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        d_model = config['d_model']\n",
    "        hidden_size_multiplier = config['hidden_size_multiplier']\n",
    "        \n",
    "        self.ff_layer = nn.Sequential(\n",
    "            nn.Linear(d_model, hidden_size_multiplier * d_model),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_size_multiplier * d_model, d_model)        \n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.ff_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9ac3000d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttentionLayer(nn.Module):\n",
    "    def __init__(self, config: Config):\n",
    "        super().__init__()\n",
    "        self.query_weights = nn.Linear(config['d_model'], config['d_model'], bias=config['qkv_bias'])\n",
    "        self.key_weights = nn.Linear(config['d_model'], config['d_model'], bias=config['qkv_bias'])\n",
    "        self.value_weights = nn.Linear(config['d_model'], config['d_model'], bias=config['qkv_bias'])\n",
    "        self.out_proj = nn.Linear(config['d_model'], config['d_model'])\n",
    "        self.num_heads = config['num_heads']\n",
    "        assert config['d_model'] % config['num_heads'] == 0, \"d_model should be divisible by num_heads\"\n",
    "        self.h_dmodel = config['d_model'] // config['num_heads']\n",
    "        self.neg_inf = - 1e+5\n",
    "        self.drop_out = torch.nn.Dropout(config['dropout'])\n",
    "        self.register_buffer('casual_mask', tensor=torch.triu(torch.ones(config['context_len'], config['context_len']), diagonal=1).bool())\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: (B, S, d_model)\n",
    "        qeury_vectors = self.query_weights(x)\n",
    "        key_vectors = self.key_weights(x)\n",
    "        value_vectors = self.value_weights(x)\n",
    "        batch_size, seq_len, d_model = x.size()\n",
    "        \n",
    "        # (B,S,d_model) -> (B, S, num_head, h_dmodel)\n",
    "        qeury_vectors = qeury_vectors.view(batch_size, seq_len, self.num_heads, self.h_dmodel)\n",
    "        key_vectors = key_vectors.view(batch_size, seq_len, self.num_heads, self.h_dmodel)\n",
    "        value_vectors = value_vectors.view(batch_size, seq_len, self.num_heads, self.h_dmodel)\n",
    "        \n",
    "        # (B, Seq, num_heads, h_dmodel) -> (B, num_heads, Seq, h_dmodel)\n",
    "        qeury_vectors = torch.permute(qeury_vectors, dims=(0, 2, 1, 3))\n",
    "        key_vectors = torch.permute(key_vectors, dims=(0, 2, 1, 3))\n",
    "        value_vectors = torch.permute(value_vectors, dims=(0, 2, 1, 3))\n",
    "        mask = self.casual_mask[ :seq_len, : seq_len]\n",
    "        \n",
    "        # mask = self.casual_mask[:seq_len, :seq_len]  # (S, S)\n",
    "        # mask = mask.unsqueeze(0).unsqueeze(0)  # (1, 1, S, S)\n",
    "         \n",
    "        attention_scores = self.calculate_attention_score(qeury_vectors, key_vectors, mask)\n",
    "        contextualized_vectores = attention_scores @ value_vectors\n",
    "        \n",
    "        # (B, num_heads, seq, head_d) => (B, seq, num_head, head_d) => (b, seq, d_model)\n",
    "        contextualized_vectores = torch.permute(contextualized_vectores, dims=(0, 2, 1, 3))\n",
    "        contextualized_vectores = contextualized_vectores.contiguous().view(batch_size, seq_len, self.num_heads*self.h_dmodel)\n",
    "        contextualized_vectores = self.out_proj(contextualized_vectores)\n",
    "        return (contextualized_vectores, attention_scores)\n",
    "    \n",
    "    def calculate_attention_score(self, qeury, key, mask):\n",
    "        # (B,NumHeads,Seq, h_dmodel) * (B,num_heads,h_model, seq) => (B,num_heads, seq, seq)\n",
    "        k_dmodel = key.size(-1)\n",
    "        attention_scores = (qeury @ key.transpose(-1,-2)) / math.sqrt(k_dmodel)\n",
    "        attention_scores = torch.masked_fill(attention_scores, mask=mask, value=self.neg_inf)\n",
    "        attention_scores = torch.softmax(attention_scores, dim=-1)\n",
    "        return self.drop_out(attention_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "eadc2639",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTDataset(Dataset):\n",
    "    def __init__(self, text, tokenizer, max_len, stride):\n",
    "        super().__init__()\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "        tokens = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "        \n",
    "        for i in range(0, len(tokens) - max_len, stride):\n",
    "            self.input_ids.append(torch.tensor(tokens[i: i + max_len]))\n",
    "            self.target_ids.append(torch.tensor(tokens[i+1 : i + max_len+1]))\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.input_ids[index], self.target_ids[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4cf1e026",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader(txt, batch_size=4, max_len=256,\n",
    "                      stride=256, shuffle=True,\n",
    "                      drop_last=True, num_workers=0):\n",
    "    \n",
    "    tokenizer = tiktoken.get_encoding('gpt2')\n",
    "    dataset = GPTDataset(text=txt, tokenizer=tokenizer, max_len=max_len, stride=stride)\n",
    "    \n",
    "    dataloader = DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        pin_memory=True,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1a6ddb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self,config: Config):\n",
    "        super().__init__()\n",
    "        self.attention = SelfAttentionLayer(config)\n",
    "        self.layer_norm1 = LayerNormalization(config)\n",
    "        self.layer_norm2 = LayerNormalization(config)\n",
    "        self.feedforward = FeedForwardLayer(config)\n",
    "        self.dropout = nn.Dropout(config['dropout'])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        shortcut = x # Residual connection\n",
    "        x = self.layer_norm1(x)\n",
    "        x, _ = self.attention(x)\n",
    "        x = self.dropout(x)\n",
    "        x = shortcut + x\n",
    "        \n",
    "        shortcut = x\n",
    "        x = self.layer_norm2(x)\n",
    "        x = self.feedforward(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x + shortcut\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5283e5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, config: Config):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(config['vocab_size'], embedding_dim=config['d_model'])\n",
    "        self.pos_embedding = nn.Embedding(config['context_len'], embedding_dim=config['d_model'])\n",
    "        self.dropout = nn.Dropout(config['dropout'])\n",
    "        \n",
    "        self.decoder_block = nn.Sequential(\n",
    "            *[TransformerBlock(config) for _ in range(config['n_layers'])]\n",
    "            )\n",
    "        \n",
    "        self.final_layernorm = LayerNormalization(config)\n",
    "        self.final_linear = nn.Linear(config['d_model'], config['vocab_size'])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len = x.size()\n",
    "        token_embedding = self.token_embedding(x)\n",
    "        positions = torch.arange(0, seq_len, device=x.device)\n",
    "        pos_embedding = self.pos_embedding(positions)\n",
    "        x = token_embedding + pos_embedding\n",
    "        x = self.decoder_block(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.final_layernorm(x)\n",
    "        logits = self.final_linear(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "244983b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = GPTModel(config=config)\n",
    "# model.token_embedding.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "eab295aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.manual_seed(10)\n",
    "# x = torch.randint(high=10,size=(1,5),dtype=torch.int)\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     out = model(x)\n",
    "# out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "89aeb411",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Generate text\n",
    "# tokenizer = tiktoken.get_encoding('gpt2')\n",
    "# def genarate_text(input, max_len, context_len):\n",
    "#     model.eval()\n",
    "#     for _ in range(max_len):\n",
    "#         input = input[:, : context_len]\n",
    "        \n",
    "#         with torch.no_grad():\n",
    "            \n",
    "#             logits = model(input)\n",
    "#             last_token_logits = logits[:, -1, :]\n",
    "#             last_token_probs = torch.softmax(last_token_logits, dim=-1)        \n",
    "#             top_prob_token = torch.argmax(last_token_probs, dim=-1, keepdim=True)\n",
    "#             input = torch.cat([input, top_prob_token], dim=-1)\n",
    "    \n",
    "#     return input\n",
    "\n",
    "# input = 'my name is'\n",
    "# input = tokenizer.encode(input)\n",
    "# input = torch.tensor(input).unsqueeze(0)\n",
    "# print(input[:, :10])\n",
    "# print(f'initial input : {input}')\n",
    "# out = genarate_text(input, max_len=5, context_len=1024)\n",
    "# print('out token', out)\n",
    "# tokenizer.decode(out.squeeze().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2c594a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read dataset\n",
    "with open('theverdict.txt','r') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "68cbb4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader = create_dataloader(text, batch_size=4, num_workers=0)\n",
    "# for batch, target in dataloader:\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ca3e84ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# model = GPTModel(config)\n",
    "# input = torch.tensor(tokenizer.encode('hai')).unsqueeze(0)\n",
    "# with torch.no_grad():\n",
    "#     print(model(input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "af3d6d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model,\n",
    "            starting_context:str='i am a good',\n",
    "            tokenizer=tokenizer,\n",
    "            max_len=10,\n",
    "            sampling=True,\n",
    "            temperature=0.0,\n",
    "            top_k=None,\n",
    "            eos_id=None):\n",
    "    \n",
    "    model.eval()\n",
    "    input_ids = tokenizer.encode(starting_context)\n",
    "    input_ids = torch.tensor(input_ids).unsqueeze(0).to(device)\n",
    "    input_ids = input_ids.to(device)\n",
    "    \n",
    "    for i in range(max_len):\n",
    "        with torch.no_grad():\n",
    "            logits = model(input_ids)\n",
    "            logits = logits[:,-1,:]\n",
    "            \n",
    "            if sampling:\n",
    "                \n",
    "                if top_k:\n",
    "                    topk_logits, topk_pos = torch.topk(logits, k=top_k, dim=-1)\n",
    "                    logits = torch.where(input=torch.tensor(float('-inf')),\n",
    "                                         condition=logits < topk_logits[:,-1].reshape(-1, 1), \n",
    "                                         other=logits)\n",
    "                if temperature>0.0:\n",
    "                    logits = logits / temperature\n",
    "                    \n",
    "                probas = F.softmax(logits, dim=-1)\n",
    "                idx_next = torch.multinomial(probas, num_samples=1)\n",
    "                input_ids = torch.concat([input_ids, idx_next], dim=-1)\n",
    "            else:\n",
    "                assert temperature==0.0 and top_k is None, \"You can't set temperature or topk if sampling=False\"\n",
    "                last_token = torch.argmax(logits, dim=-1).unsqueeze(0)\n",
    "                input_ids = torch.cat([input_ids, last_token], dim=-1)\n",
    "    return tokenizer.decode(input_ids.squeeze().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6b915027",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "def train(model, train_dataloader:DataLoader, val_dataloader, optimizer:AdamW, epochs, val_freq:int, num_iter:int, device_man:str):\n",
    "    if device_man:\n",
    "        model.to(device_man)\n",
    "    else:\n",
    "        model.to(device)\n",
    "        \n",
    "    global_step = 1\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss_history = []\n",
    "        val_loss_history = []\n",
    "        \n",
    "        for batch, target in train_dataloader:\n",
    "            batch, target = batch.to(device), target.to(device)\n",
    "            loss = calculate_batch_loss(model, batch, target)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            global_step+=1\n",
    "            \n",
    "            # validation\n",
    "            if global_step % val_freq == 0:\n",
    "                train_loss, val_loss = evaluate_model(model, train_dataloader, val_dataloader, num_iter)\n",
    "                print(f\"epoch: {epoch}: train_loss: {train_loss:.3f}, val_loss:{val_loss:.3f}\")\n",
    "                train_loss_history.append(train_loss)\n",
    "                val_loss_history.append(val_loss)\n",
    "                print(\"sample generation: \", generate(model))\n",
    "            \n",
    " \n",
    "def calculate_batch_loss(model, batch, target):\n",
    "    logits = model(batch).flatten(0, 1)\n",
    "    target = target.flatten()\n",
    "    loss = torch.nn.functional.cross_entropy(logits, target)\n",
    "    return loss\n",
    "\n",
    "def evaluate_model(model, train_dataloader, val_dataloader, num_iter):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = calculate_dataloader_loss(model, train_dataloader, num_iter=num_iter)\n",
    "        val_loss = calculate_dataloader_loss(model, val_dataloader, num_iter=num_iter)\n",
    "    model.train()\n",
    "    return train_loss, val_loss\n",
    "               \n",
    "def calculate_dataloader_loss(model, dataloader, num_iter):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    for batch, target in dataloader:\n",
    "        batch, target = batch.to(device), target.to(device)\n",
    "        out = model(batch) \n",
    "        loss = torch.nn.functional.cross_entropy(out.flatten(0, 1), target=target.flatten())\n",
    "        total_loss += loss.item() \n",
    "   \n",
    "    avg_loss = total_loss/num_iter\n",
    "    return avg_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1fad77c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data\n",
    "train_data = tokenizer.encode(text)[:int(0.8 * len(tokenizer.encode(text)))]\n",
    "val_data = tokenizer.encode(text)[int(0.8 * len(tokenizer.encode(text))):]\n",
    "\n",
    "train_text = tokenizer.decode(train_data)\n",
    "val_text = tokenizer.decode(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "539dbd1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPTModel(config)\n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b0b0bf82",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = create_dataloader(txt=train_text, batch_size=4, max_len=256, stride=256)\n",
    "val_dataloader = create_dataloader(txt=val_text, batch_size=2, max_len=256, stride=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "fd8ba036",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sympy.printing.numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[56]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m optimizer = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptim\u001b[49m\u001b[43m.\u001b[49m\u001b[43mAdam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5e-4\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda\\Lib\\site-packages\\torch\\optim\\adam.py:100\u001b[39m, in \u001b[36mAdam.__init__\u001b[39m\u001b[34m(self, params, lr, betas, eps, weight_decay, amsgrad, foreach, maximize, capturable, differentiable, fused, decoupled_weight_decay)\u001b[39m\n\u001b[32m     85\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mTensor betas[1] must be 1-element\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     87\u001b[39m defaults = \u001b[38;5;28mdict\u001b[39m(\n\u001b[32m     88\u001b[39m     lr=lr,\n\u001b[32m     89\u001b[39m     betas=betas,\n\u001b[32m   (...)\u001b[39m\u001b[32m     98\u001b[39m     decoupled_weight_decay=decoupled_weight_decay,\n\u001b[32m     99\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m100\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefaults\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    102\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m fused:\n\u001b[32m    103\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m differentiable:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda\\Lib\\site-packages\\torch\\optim\\optimizer.py:369\u001b[39m, in \u001b[36mOptimizer.__init__\u001b[39m\u001b[34m(self, params, defaults)\u001b[39m\n\u001b[32m    366\u001b[39m     param_groups = [{\u001b[33m\"\u001b[39m\u001b[33mparams\u001b[39m\u001b[33m\"\u001b[39m: param_groups}]\n\u001b[32m    368\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m param_group \u001b[38;5;129;01min\u001b[39;00m param_groups:\n\u001b[32m--> \u001b[39m\u001b[32m369\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43madd_param_group\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_group\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    371\u001b[39m \u001b[38;5;66;03m# Allows _cuda_graph_capture_health_check to rig a poor man's TORCH_WARN_ONCE in python,\u001b[39;00m\n\u001b[32m    372\u001b[39m \u001b[38;5;66;03m# which I don't think exists\u001b[39;00m\n\u001b[32m    373\u001b[39m \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/issues/72948\u001b[39;00m\n\u001b[32m    374\u001b[39m \u001b[38;5;28mself\u001b[39m._warned_capturable_if_run_uncaptured = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda\\Lib\\site-packages\\torch\\_compile.py:46\u001b[39m, in \u001b[36m_disable_dynamo.<locals>.inner\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     44\u001b[39m disable_fn = \u001b[38;5;28mgetattr\u001b[39m(fn, \u001b[33m\"\u001b[39m\u001b[33m__dynamo_disable\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m disable_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_dynamo\u001b[39;00m\n\u001b[32m     48\u001b[39m     disable_fn = torch._dynamo.disable(fn, recursive)\n\u001b[32m     49\u001b[39m     fn.__dynamo_disable = disable_fn  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda\\Lib\\site-packages\\torch\\_dynamo\\__init__.py:13\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[33;03mTorchDynamo is a Python-level JIT compiler designed to make unmodified PyTorch programs faster.\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[33;03mTorchDynamo hooks into the frame evaluation API in CPython (PEP 523) to dynamically modify Python\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m      8\u001b[39m \u001b[33;03mseamlessly optimize PyTorch programs, including those using modern Python features.\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[34;01mtorch\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config, convert_frame, eval_frame, resume_execution\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01m.\u001b[39;00m\u001b[34;01mbackends\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mregistry\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m list_backends, lookup_backend, register_backend\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01m.\u001b[39;00m\u001b[34;01mcallback\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m callback_handler, on_compile_end, on_compile_start\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:52\u001b[39m\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_C\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_dynamo\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mguards\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GlobalStateGuard\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_dynamo\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdistributed\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_compile_pg\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_dynamo\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msymbolic_convert\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TensorifyState\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_guards\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compile_context, CompileContext, CompileId, tracing\n\u001b[32m     54\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_logging\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m structured\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py:52\u001b[39m\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[34;01mtorch\u001b[39;00m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_logging\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_dynamo\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TensorifyScalarRestartAnalysis\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_guards\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tracing, TracingContext\n\u001b[32m     54\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfx\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexperimental\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msymbolic_shapes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m guard_bool\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda\\Lib\\site-packages\\torch\\_dynamo\\exc.py:41\u001b[39m\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_guards\u001b[39;00m\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m counters\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n\u001b[32m     45\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[34;01mtypes\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda\\Lib\\site-packages\\torch\\_dynamo\\utils.py:69\u001b[39m\n\u001b[32m     67\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[34;01mtorch\u001b[39;00m\n\u001b[32m     68\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_functorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfig\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m69\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfx\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexperimental\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msymbolic_shapes\u001b[39;00m\n\u001b[32m     70\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_pytree\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[34;01mpytree\u001b[39;00m\n\u001b[32m     71\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m fx\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda\\Lib\\site-packages\\torch\\fx\\experimental\\symbolic_shapes.py:67\u001b[39m\n\u001b[32m     65\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_ordered_set\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OrderedSet\n\u001b[32m     66\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_python_dispatch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_traceable_wrapper_subclass\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_sympy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfunctions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     68\u001b[39m     Application,\n\u001b[32m     69\u001b[39m     CeilToInt,\n\u001b[32m     70\u001b[39m     CleanDiv,\n\u001b[32m     71\u001b[39m     FloorDiv,\n\u001b[32m     72\u001b[39m     FloorToInt,\n\u001b[32m     73\u001b[39m     IsNonOverlappingAndDenseIndicator,\n\u001b[32m     74\u001b[39m     Max,\n\u001b[32m     75\u001b[39m     Mod,\n\u001b[32m     76\u001b[39m     PythonMod,\n\u001b[32m     77\u001b[39m )\n\u001b[32m     78\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_sympy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnumbers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m int_oo\n\u001b[32m     79\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_sympy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mprinters\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CppPrinter, PythonPrinter\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda\\Lib\\site-packages\\torch\\utils\\_sympy\\functions.py:9\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Callable, Optional, SupportsFloat, TYPE_CHECKING, TypeVar, Union\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01mtyping_extensions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TypeVarTuple, Unpack\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[34;01msympy\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01msympy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m S\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01msympy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sympify\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda\\Lib\\site-packages\\sympy\\__init__.py:74\u001b[39m\n\u001b[32m     67\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01m.\u001b[39;00m\u001b[34;01mlogic\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (to_cnf, to_dnf, to_nnf, And, Or, Not, Xor, Nand, Nor,\n\u001b[32m     68\u001b[39m         Implies, Equivalent, ITE, POSform, SOPform, simplify_logic, bool_map,\n\u001b[32m     69\u001b[39m         true, false, satisfiable)\n\u001b[32m     71\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01m.\u001b[39;00m\u001b[34;01massumptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (AppliedPredicate, Predicate, AssumptionsContext,\n\u001b[32m     72\u001b[39m         assuming, Q, ask, register_handler, remove_handler, refine)\n\u001b[32m---> \u001b[39m\u001b[32m74\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01m.\u001b[39;00m\u001b[34;01mpolys\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (Poly, PurePoly, poly_from_expr, parallel_poly_from_expr,\n\u001b[32m     75\u001b[39m         degree, total_degree, degree_list, LC, LM, LT, pdiv, prem, pquo,\n\u001b[32m     76\u001b[39m         pexquo, div, rem, quo, exquo, half_gcdex, gcdex, invert,\n\u001b[32m     77\u001b[39m         subresultants, resultant, discriminant, cofactors, gcd_list, gcd,\n\u001b[32m     78\u001b[39m         lcm_list, lcm, terms_gcd, trunc, monic, content, primitive, compose,\n\u001b[32m     79\u001b[39m         decompose, sturm, gff_list, gff, sqf_norm, sqf_part, sqf_list, sqf,\n\u001b[32m     80\u001b[39m         factor_list, factor, intervals, refine_root, count_roots, all_roots,\n\u001b[32m     81\u001b[39m         real_roots, nroots, ground_roots, nth_power_roots_poly, cancel,\n\u001b[32m     82\u001b[39m         reduced, groebner, is_zero_dimensional, GroebnerBasis, poly,\n\u001b[32m     83\u001b[39m         symmetrize, horner, interpolate, rational_interpolate, viete, together,\n\u001b[32m     84\u001b[39m         BasePolynomialError, ExactQuotientFailed, PolynomialDivisionFailed,\n\u001b[32m     85\u001b[39m         OperationNotSupported, HeuristicGCDFailed, HomomorphismFailed,\n\u001b[32m     86\u001b[39m         IsomorphismFailed, ExtraneousFactors, EvaluationFailed,\n\u001b[32m     87\u001b[39m         RefinementFailed, CoercionFailed, NotInvertible, NotReversible,\n\u001b[32m     88\u001b[39m         NotAlgebraic, DomainError, PolynomialError, UnificationFailed,\n\u001b[32m     89\u001b[39m         GeneratorsError, GeneratorsNeeded, ComputationFailed,\n\u001b[32m     90\u001b[39m         UnivariatePolynomialError, MultivariatePolynomialError,\n\u001b[32m     91\u001b[39m         PolificationFailed, OptionError, FlagError, minpoly,\n\u001b[32m     92\u001b[39m         minimal_polynomial, primitive_element, field_isomorphism,\n\u001b[32m     93\u001b[39m         to_number_field, isolate, round_two, prime_decomp, prime_valuation,\n\u001b[32m     94\u001b[39m         galois_group, itermonomials, Monomial, lex, grlex,\n\u001b[32m     95\u001b[39m         grevlex, ilex, igrlex, igrevlex, CRootOf, rootof, RootOf,\n\u001b[32m     96\u001b[39m         ComplexRootOf, RootSum, roots, Domain, FiniteField, IntegerRing,\n\u001b[32m     97\u001b[39m         RationalField, RealField, ComplexField, PythonFiniteField,\n\u001b[32m     98\u001b[39m         GMPYFiniteField, PythonIntegerRing, GMPYIntegerRing, PythonRational,\n\u001b[32m     99\u001b[39m         GMPYRationalField, AlgebraicField, PolynomialRing, FractionField,\n\u001b[32m    100\u001b[39m         ExpressionDomain, FF_python, FF_gmpy, ZZ_python, ZZ_gmpy, QQ_python,\n\u001b[32m    101\u001b[39m         QQ_gmpy, GF, FF, ZZ, QQ, ZZ_I, QQ_I, RR, CC, EX, EXRAW,\n\u001b[32m    102\u001b[39m         construct_domain, swinnerton_dyer_poly, cyclotomic_poly,\n\u001b[32m    103\u001b[39m         symmetric_poly, random_poly, interpolating_poly, jacobi_poly,\n\u001b[32m    104\u001b[39m         chebyshevt_poly, chebyshevu_poly, hermite_poly, hermite_prob_poly,\n\u001b[32m    105\u001b[39m         legendre_poly, laguerre_poly, apart, apart_list, assemble_partfrac_list,\n\u001b[32m    106\u001b[39m         Options, ring, xring, vring, sring, field, xfield, vfield, sfield)\n\u001b[32m    108\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01m.\u001b[39;00m\u001b[34;01mseries\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (Order, O, limit, Limit, gruntz, series, approximants,\n\u001b[32m    109\u001b[39m         residue, EmptySequence, SeqPer, SeqFormula, sequence, SeqAdd, SeqMul,\n\u001b[32m    110\u001b[39m         fourier_series, fps, difference_delta, limit_seq)\n\u001b[32m    112\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01m.\u001b[39;00m\u001b[34;01mfunctions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (factorial, factorial2, rf, ff, binomial,\n\u001b[32m    113\u001b[39m         RisingFactorial, FallingFactorial, subfactorial, carmichael,\n\u001b[32m    114\u001b[39m         fibonacci, lucas, motzkin, tribonacci, harmonic, bernoulli, bell, euler,\n\u001b[32m   (...)\u001b[39m\u001b[32m    135\u001b[39m         Znm, elliptic_k, elliptic_f, elliptic_e, elliptic_pi, beta, mathieus,\n\u001b[32m    136\u001b[39m         mathieuc, mathieusprime, mathieucprime, riemann_xi, betainc, betainc_regularized)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda\\Lib\\site-packages\\sympy\\polys\\__init__.py:94\u001b[39m\n\u001b[32m     82\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01m.\u001b[39;00m\u001b[34;01mrationaltools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m together\n\u001b[32m     84\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01m.\u001b[39;00m\u001b[34;01mpolyerrors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (BasePolynomialError, ExactQuotientFailed,\n\u001b[32m     85\u001b[39m         PolynomialDivisionFailed, OperationNotSupported, HeuristicGCDFailed,\n\u001b[32m     86\u001b[39m         HomomorphismFailed, IsomorphismFailed, ExtraneousFactors,\n\u001b[32m   (...)\u001b[39m\u001b[32m     91\u001b[39m         MultivariatePolynomialError, PolificationFailed, OptionError,\n\u001b[32m     92\u001b[39m         FlagError)\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01m.\u001b[39;00m\u001b[34;01mnumberfields\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (minpoly, minimal_polynomial, primitive_element,\n\u001b[32m     95\u001b[39m         field_isomorphism, to_number_field, isolate, round_two, prime_decomp,\n\u001b[32m     96\u001b[39m         prime_valuation, galois_group)\n\u001b[32m     98\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01m.\u001b[39;00m\u001b[34;01mmonomials\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m itermonomials, Monomial\n\u001b[32m    100\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01m.\u001b[39;00m\u001b[34;01morderings\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m lex, grlex, grevlex, ilex, igrlex, igrevlex\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda\\Lib\\site-packages\\sympy\\polys\\numberfields\\__init__.py:21\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01m.\u001b[39;00m\u001b[34;01mminpoly\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m minpoly, minimal_polynomial\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01m.\u001b[39;00m\u001b[34;01msubfield\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m field_isomorphism, primitive_element, to_number_field\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01m.\u001b[39;00m\u001b[34;01mutilities\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m isolate\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01m.\u001b[39;00m\u001b[34;01mbasis\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m round_two\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01m.\u001b[39;00m\u001b[34;01mprimes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m prime_decomp, prime_valuation\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda\\Lib\\site-packages\\sympy\\polys\\numberfields\\utilities.py:9\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01msympy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpolys\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmatrices\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DMRankError\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01msympy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpolys\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnumberfields\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mminpoly\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m minpoly\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01msympy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mprinting\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlambdarepr\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m IntervalPrinter\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01msympy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutilities\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdecorator\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m public\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01msympy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutilities\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlambdify\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m lambdify\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda\\Lib\\site-packages\\sympy\\printing\\lambdarepr.py:5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01m.\u001b[39;00m\u001b[34;01mpycode\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      2\u001b[39m     PythonCodePrinter,\n\u001b[32m      3\u001b[39m     MpmathPrinter,\n\u001b[32m      4\u001b[39m )\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01m.\u001b[39;00m\u001b[34;01mnumpy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NumPyPrinter  \u001b[38;5;66;03m# NumPyPrinter is imported for backward compatibility\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01msympy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msorting\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m default_sort_key\n\u001b[32m      9\u001b[39m __all__ = [\n\u001b[32m     10\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mPythonCodePrinter\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     11\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mMpmathPrinter\u001b[39m\u001b[33m'\u001b[39m,  \u001b[38;5;66;03m# MpmathPrinter is published for backward compatibility\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     16\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mlambdarepr\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     17\u001b[39m ]\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'sympy.printing.numpy'"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907cf30c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0: train_loss: 7.646, val_loss:3.971\n",
      "sample generation:  i am a good exhibitedly Pipe RISJasonaved tir Northwestern Suz Moines\n",
      "epoch: 0: train_loss: 6.884, val_loss:3.604\n",
      "sample generation:  i am a good Rivers Garrison monkeys parallelsabi Proto Gordon horr contraception,\n",
      "epoch: 1: train_loss: 6.161, val_loss:3.307\n",
      "sample generation:  i am a goodussion--utral nail Safari0000000000000000 told utter straw insisted\n",
      "epoch: 1: train_loss: 5.505, val_loss:3.043\n",
      "sample generation:  i am a good elaboratewing Meanwhile. precious-- theConclusion unions explaining\n",
      "epoch: 2: train_loss: 4.955, val_loss:2.852\n",
      "sample generation:  i am a good Included me depicting look Taylor and to of Trem Jack\n",
      "epoch: 2: train_loss: 10.414, val_loss:5.590\n",
      "sample generation:  i am a good unspecified�� pigment disqualified illusion unlawfullyuture qualifying watering magnitude\n",
      "epoch: 3: train_loss: 4.398, val_loss:2.729\n",
      "sample generation:  i am a goodImproved of theENDEDesa myblind Burlington him!\n",
      "epoch: 3: train_loss: 4.107, val_loss:2.795\n",
      "sample generation:  i am a good family departments pessimistic apologyagged in little Chic Dharma He\n",
      "epoch: 4: train_loss: 3.847, val_loss:2.741\n",
      "sample generation:  i am a good,\" accordingement breeding gar the host ballots.\"\n",
      "\n",
      "epoch: 4: train_loss: 3.584, val_loss:2.738\n",
      "sample generation:  i am a goodBig \"[]Bird he had meitation him,\n",
      "epoch: 5: train_loss: 3.286, val_loss:2.736\n",
      "sample generation:  i am a goodMrs trade \"river you buyingeredseen facechairs\n",
      "epoch: 5: train_loss: 3.029, val_loss:2.697\n",
      "sample generation:  i am a goodwidth you += believed a little sorry's\n",
      "I\n",
      "epoch: 6: train_loss: 2.814, val_loss:2.698\n",
      "sample generation:  i am a good. had been RP. And, too wildfire dab\n",
      "epoch: 6: train_loss: 2.590, val_loss:2.673\n",
      "sample generation:  i am a good conserve ay one ranch with him Cold up out\n",
      "epoch: 7: train_loss: 2.352, val_loss:2.661\n",
      "sample generation:  i am a good-one of they that modesty, terr? drawing\n",
      "epoch: 7: train_loss: 2.104, val_loss:2.683\n",
      "sample generation:  i am a good reconsStates was when that themicrosoft he saidn\n",
      "epoch: 8: train_loss: 1.903, val_loss:2.664\n",
      "sample generation:  i am a good- it. To caught if.\"\n",
      "\n",
      "\n",
      "\n",
      "epoch: 8: train_loss: 1.655, val_loss:2.689\n",
      "sample generation:  i am a goodcknow glad Jennysuccess sounded-- showed heBatmanitle\n",
      "epoch: 9: train_loss: 1.457, val_loss:2.692\n",
      "sample generation:  i am a good?\" I rainway nothing pedoph waves affect a in\n",
      "epoch: 9: train_loss: 1.260, val_loss:2.710\n",
      "sample generation:  i am a good exploits pinkany's domestic my wanted statues thought represented\n"
     ]
    }
   ],
   "source": [
    "train(model=model,\n",
    "      train_dataloader=train_dataloader,\n",
    "      val_dataloader=val_dataloader,\n",
    "      optimizer=optimizer,\n",
    "      epochs=10,\n",
    "      val_freq=2,\n",
    "      num_iter=5,\n",
    "      device_man=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68989a26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'you should beauty into circulation,\" was not that the axioms he had seen a degree to be that when'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(model=model, starting_context=\"you should\", tokenizer=tokenizer, max_len=20, temperature=.8, top_k=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aec7e45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08267628",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac88bcaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt_download import download_and_load_gpt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327115fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "checkpoint: 100%|██████████| 77.0/77.0 [00:00<?, ?iB/s]\n",
      "encoder.json: 100%|██████████| 1.04M/1.04M [00:02<00:00, 519kiB/s] \n",
      "hparams.json: 100%|██████████| 90.0/90.0 [00:00<?, ?iB/s]\n",
      "model.ckpt.data-00000-of-00001: 100%|██████████| 498M/498M [04:30<00:00, 1.84MiB/s]   \n",
      "model.ckpt.index: 100%|██████████| 5.21k/5.21k [00:00<00:00, 2.50MiB/s]\n",
      "model.ckpt.meta: 100%|██████████| 471k/471k [00:01<00:00, 294kiB/s]  \n",
      "vocab.bpe: 100%|██████████| 456k/456k [00:01<00:00, 238kiB/s]  \n"
     ]
    }
   ],
   "source": [
    "settigns, params = download_and_load_gpt2(model_size='124M', models_dir='gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7eb5237",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_vocab': 50257, 'n_ctx': 1024, 'n_embd': 768, 'n_head': 12, 'n_layer': 12}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "settigns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d79766",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
